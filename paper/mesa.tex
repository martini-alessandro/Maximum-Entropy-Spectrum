\documentclass[twocolumn,showpacs,preprintnumbers,nofootinbib,prd,
superscriptaddress,10pt]{revtex4-1}

\usepackage{amsmath,amssymb}
\usepackage{amsfonts}
\usepackage[normalem]{ulem}
\usepackage{textcomp}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{bm}
\usepackage{afterpage}
\usepackage{graphicx}
\usepackage{psfrag}
\usepackage{mathtools}
%\usepackage{tensor}
\usepackage{layouts}
%\usepackage{DejaVuSans}
\usepackage{epstopdf}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[utf8]{inputenc}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{tabularx}
\usepackage{ragged2e}
\usepackage{cleveref}
\usepackage{blindtext}
\usepackage{graphicx}
\usepackage{siunitx}
	\sisetup{output-decimal-marker={.}}
\newtheorem{theorem}{Theorem}
\usepackage{float}
	

	%some math symbols
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\DeclareMathOperator{\sign}{sign}
\renewcommand{\d}[1]{\ensuremath{\operatorname{d}\!{#1}}}
%argmin and argmax
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% comments command
\newcommand{\wdp}[1]{{\textcolor{magenta}{\texttt{WDP: #1}} }}
\newcommand{\amartini}[1]{{\textcolor{blue}{\texttt{AM: #1}} }}
\newcommand{\sschmidt}[1]{{\textcolor{red}{\texttt{SS: #1}} }}

\begin{document}

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ABSTRACT
\begin{abstract}
The Maximum Entropy Spectral Analysis (MESA) method, developed by Burg, provides a powerful tool to perform spectral estimation of a time-series. The method relies on a Jaynes' maximum entropy principle and provides the means of inferring the spectrum of a stochastic process in terms of the coefficients of some autoregressive process AR($p$) of order $p$.
A closed form recursive solution provides an estimate of the autoregressive coefficients as well as of the order $p$ of the process.
We provide a ready-to-use implementation of the algorithm in the form of a python package \texttt{memspectrum}. We characterize our implementation by performing a power spectral density analysis on synthetic data (with known power spectral density) and we compare different criteria for stopping the recursion. Furthermore, we compare the performance of our code with the ubiquitous Welch algorithm, using synthetic data generated from the released spectrum by the LIGO-Virgo collaboration.
We find that, when compared to Welch's method, Burg's method provides a power spectral density (PSD) estimation with a 
systematically lower variance and bias. This is particularly manifest in the case of a little number of data points, making Burg's method most suitable to work in this regime.

\end{abstract}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TITLE
	\title{Maximum Entropy Spectral Analysis: a case study}
	\author{Alessandro \surname{Martini}}
		\email{martini.alessandr@gmail.com}
        \affiliation{Dipartimento di Fisica  Università di Pisa, and INFN Sezione di Pisa, Pisa I-56127,Italy}        
	\author{Stefano \surname{Schmidt}}
		\email{s.schmidt@uu.nl}
        \affiliation{Institute for Gravitational and Subatomic Physics (GRASP), Utrecht University, Princetonplein 1, 3584 CC, Utrecht, The Netherlands}      
	\author{Walter \surname{Del Pozzo}}
		\email{walter.delpozzo@unipi.it}
        \affiliation{Dipartimento di Fisica  Università di Pisa, and INFN Sezione di Pisa, Pisa I-56127,Italy}      
  
	
	\maketitle
	%\tableofcontents %comment this to hide the table of content

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BODY  

\section{Introduction}

The study of the properties of stochastic processes is a crucial task in many fields of physics, astronomy, quantitative biology, as well as engineering and finance. Among those, a special place is occupied by the so-called \textit{wide-sense} stationary processes. These are stochastic processes that display an invariance of their statistical properties, such as their two-point autocovariance function, with respect to time translation. If $x(t)$ is a wide-sense stationary process, it is completely determined by the knowledge of the autocorrelation function 
\begin{equation}
	C(\tau) = \mathbf{E}[x_t \cdot x_{t+\tau}]
\end{equation}
or, equivalently, by the knowledge of their \emph{power spectral density} (PSD) $S(f)$. Thanks to the Wiener-Khinchin theorem the two are related by a Fourier transform: 
\begin{equation}\label{eq:psd-autocorrelation}
	S(f) = \int_{-\infty}^{\infty} \textrm{d}\tau C(\tau) e^{-i 2 \pi f \tau}\,.
\end{equation}
In some literature, especially in the context of gravitational waves physics, e.g. ~\cite{Finn_1992}, the PSD is introduced as  
\begin{equation}\label{eq:psd-f-definition}
	\mathbf{E}[\tilde{x}(f) \cdot \tilde{x}(f^\prime)] = S(f) \delta(f-f^\prime)
\end{equation}
without highlighting its connection with the time structure of the process itself, thus masking some important properties that will explore further in what follows. The latter definition in~\eqref{eq:psd-f-definition} gives, however, i) a straightforward interpretation of the PSD: it measures how much signal ``power" is located in each frequency; ii) an operative way of estimating it for an unknown process.  

An ubiquitous method for such computation is due to Welch \cite{Welch1967} and it based on eqs.(\ref{eq:psd-autocorrelation}-\ref{eq:psd-f-definition}).
The PSD is obtained by slicing the observed realization $x(t_1),\ldots,x(t_n)$ of the process $x(t)$ into many window-corrected batches and averaging the squared moduli of their Fourier transforms.
This approach is equivalent~\cite{Lomb,Scargle} to taking the Fourier Transform of the windowed sample autocorrelation $\rho_W$, written as
\begin{equation}
    \rho_{W} = \left\{W_0\rho_0,W_{\pm 1}\rho_{\pm 1}, \dots, W_{\pm M}\rho_{\pm M}, 0, 0, \dots \right\},
\end{equation}
where $\rho$ is the empirical autocorrelation and $M$ is the maximum time lag at which the autocorrelation is computed.
The sequence $W$ is a window function that can be chosen in several different way, each choice presenting advantages and disadvantages for the final estimate of the PSD.

The choice of a window function is arbitrary and typically is made by trial and error, until a satisfactory compromise between variance and resolution of the estimate of PSD is reached. A high frequency resolution implies high variance and vice-versa.
Besides the window function, Welch's method requires a number of arbitrary choices to be made, such as the number of time slices and the overlap between consecutive slices. All these knobs must be tuned by hand and their choice can dramatically affect the PSD estimation, hence begging the question of what the ``best" PSD estimate is.

Another problem with this approach is the requirement for the window to be $0$ outside the interval in which the autocorrelation is computed.
We are arbitrarily assuming $\rho_j = 0$ for $j > M$ and modifying the estimate (i.e. the data) if a non-rectangular window is chosen.
Making assumptions on unobserved data and modifying the ones we have at our disposal introduces ``spurious" information about the process that we, in general, do not really have.

An appealing alternative approach, based on the Maximum Entropy principle \cite{JaynesArticle,jaynes2003ptl, Jaynes_MAXENT}, has been put forward by Burg \cite{burg1975maximum}. Being rooted on solid theoretical foundations, we will see that Burg's method, unlike Welch's, does not require any preprocessing of the data and requires very little tuning of the algorithm parameters, since it provides an iterative closed form expression for the spectrum of a stochastic stationary time series. Furthermore, it embeds the PSD estimation problem into an elegant theoretical framework and makes minimal assumptions on the nature of the data.
Lastly and most importantly, it provides a robust link between spectral density estimation and the field of autoregressive processes. This provides a natural and simple machinery to forecast a time series, thus predicting future observations based on previous ones.

In this paper, we discuss the details of the Maximum entropy principle, its application to the problem of PSD estimation with Burg's algorithm and the link between Burg's algorithm and autoregressive process.
Our goal is to bring (again) to public attention Maximum Entropy Spectral analysis, in the hope that it will be widely employed as a way out to the many problems posed by the Welch's algorithm (or other similar methods).
To facilitate this goal, we present and describe a new code, \texttt{memspectrum}, that provides a robust and easy-to-use python implementation of the algorithm\footnote{
It is available at link: \url{https://pypi.org/project/memspectrum/}.
}.
We provide a thorough assessment of the performance of our code and we validate our results performing a number of tests on simulated and real data.
We also compare our results with those of spectral analysis carried out with the standard Welch's method.
In order to apply our model on a realistic setting, we analyse some time series of broad interest in the scientific community.

Our paper is organized as follows: we being by briefly reviewing the theoretical foundations of the maximum entropy principle in Sec.~\ref{sec:foundations}. Sec.~\ref{sec:validation} presents the validation of Burg's method as well as of our implementation on simulated data. In Sec.~\ref{sec:Welch_comparison} we compare the results from \texttt{memspectrum} with the Welch method; Sec.~\ref{sec:applications} presents a few applications to real time series and, finally, we conclude with a discussion in Sec.~\ref{sec:conclusion}.


\section{Theoretical foundations}\label{sec:foundations}
The Maximum Entropy principle (MAXENT) is among the most important results in probability theory. It provides a way to uniquely assign probabilities to a phenomenon in a way that best represent our state of knowledge, while being non committal with unavailable information. Its domain of application turned out to be wider than expected. In fact, thanks to Burg \cite{burg1975maximum}, this method has also been applied to perform high quality computation of power spectral densities of time series.

After a short introduction to Jayne's MAXENT (sec.~\ref{sec:MAXENT}), we will develop in detail Burg's technique of Maximum Entropy Spectral Analysis (MESA) and show that the estimate can be expressed in an analytical closed form (sec.~\ref{sec:MESA}).
Next, we will discuss an interesting link between Burg's method and autoregressive processes (sec.~\ref{sec:autoregr}) and in sec.~\ref{sec:forecasting} we will use such link for straightforwardly forecasting a time series.

\subsection{Maximum Entropy Principle} \label{sec:MAXENT}

Before introducing MAXENT principle, we will define through some simple examples the two core concepts of the problem and the role they play: the `evidence' and the `information'.
Let us start with the `information' (or entropy): it is a measure of the degree of uncertainty on the outcomes of some experiment and specifies the length of the message necessary to provide a full description of the system under study. For instance, consider a perfectly sinusoidal signal: knowledge of amplitude, frequency and phase are sufficient to fully reproduce it: it has a low information content. Even less information is required if we are studying a system whose outcome is certain (has probability $p = 1$), as in this case, a communication is not even needed.  
Shannon \cite{Shannon} proposed the quantity
\begin{equation}\label{eq:information}
    I = \log_2 \frac{1}{p(x)}
\end{equation}
to measure the quantity of information brought by an outcome $x$ with probability $p(x)$. It is additive quantity as well as monotonically decreasing as a function of $p \in [0, 1]$: the more uncertain the outcome, the higher the information it brings.

We can generalize the definition of information in the case where two different outcomes $E_1, E_2$, with given probabilities $P_1$ and $P_2$, are possible.
To gain some intuition on the problem, we ask ourselves which are the probability assignments that makes the outcome more uncertain (i.e. maximize the information).
If $P_1$ and $P_2$ are largely different, for instance $P_1 = 0.999$ and $P_2 = 0.001$, we are allowed to believe that event $E_1$ will occur almost certainly, considering $E_2$ to be a very implausible outcome. The information content will be very low.
On the other hand, most unpredictable experiment happens when 
\begin{equation}\nonumber
    P_1 = P_2 = \frac{1}{2}:
\end{equation}
this describes a situation of `maximum ignorance' and the information content of such system must be high.
Any generalization of eq.~\eqref{eq:information}, must then have its maximum when $ P_1 = P_2$.
For $N$ events, system with the highest possible information content is when:
\begin{equation}\nonumber
    P_1 = \hdots = P_N = \frac{1}{N}:
\end{equation}

Shannon \cite{Shannon} showed that the only functional form satisfying continuity with respect to its parameters, additivity and that has a maximum for equal probability events is:
\begin{equation}\label{eq:entropy}
    H[p_1, \dots, p_N] = - \sum_{i = 1}^N p_i\log{p_i},
\end{equation}
The equation can be interpreted as the `expected information' brought by an experiment with $N$ possible outcomes each with its own probability $p_i$.
In the continuous case:
\begin{equation} \label{eq:entropy_continuos}
    H[p(x)] = - \int p(x)\ln p(x) dx,
\end{equation}
We call the functional $H$ (information) entropy\footnote{In defining the information entropy as in Eq.~(\ref{eq:entropy_continuos}) we are implicitly assuming a uniform measure over the parameter space}.

We now turn the core of our problem: how do we make a probability assignment for a set of events, that keeps into account our knowledge of the system and, at the same time, it is non committal towards unavailable knowledge?
The knowledge at our disposal about the system under study is what we call `evidence' and any probability assignment must agree with it.
In the above cases, our knowledge on the system is only the total number $N$ of different outcomes -- this is a minimal requirement. Of course, more complex evidence constraints can be applied.

It is very common that the constraints provided by the evidence are not enough for setting the probabilities for each event: in this case, it is reasonable to assume that the probability assignment should make the experiment as unpredictable as possible\footnote{
In ref.~\cite{Jaynes_MAXENT} this statement is made more precise and justified more thoroughly, with arguments based on combinatorial analysis.
}.
In other words, the amount of `information' introduced by the the probability assignment should be as high as possible.

MAXENT puts the aforementioned principle into a more precise mathematical context by stating that probabilities should be assigned by maximizing uncertainty (information entropy) using evidence as constraint. 
This defines a variational problem, where the information entropy functional $H\left[p_1, \dots, p_N\right]$, defined in eq.~\eqref{eq:entropy}, has to maximized. 

The maximisation of entropy, supplemented by evidence in the form of constraints to which the sought-for probability distribution must obey,
gives rise to several of the most common probability distributions commonly employed in statistics. For instance, whenever the only constraint available is the normalization of the probability distribution (i.e. no evidence is available), the entropy is maximised by the uniform distribution. 
If we have evidence to constraint the expected value, the information entropy is maximised by the exponential distribution

Of particular importance is the case in which, in addition to the mean, also the variance is known: MAXENT leads to the Gaussian distribution. 
We retain this particularly interesting from the foundational point of view, since it provides a deeper insight into the ubiquitous Gaussian distribution.
Indeed, it is not only the limit distribution provided by the central limit theorem for finite variance processes but it is also the distribution that maximizes the entropy. For this reason, appealing to MAXENT principle, it is the correct assignment if mean and covariance are the only quantities that fully define our process. In some sense, we can interpret the central limit theorem as the natural `statistical' evolution toward a configuration that maximizes entropy.

For this work, we are particularly interested in the multi-dimensional case. Suppose we have a vector of measurements $(x(t_1),\ldots,x(t_n)) = (x_1, \ldots, x_n)$ that we conveniently express as a single realization of an unknown stochastic process $x(t)$ and we have information about the expectation value of the process $\mu(t)$ and on the matrix of autocovariances $C_{ij} \equiv C(t_i,t_j)$, then the MAXENT distribution is the $n$-dimensional multivariate Gaussian distribution~\cite{gregory_2005}: 
\begin{align}
    p&\left((x_1, \ldots, x_n)\vert I\right) = \nonumber \\
    &\frac{1}{\left(2 \pi \det C\right)^{k / 2}}\exp\left(-\frac{1}{2}\sum_{i,j}(x_i-\mu_i) (x_j-\mu_j)C^{-1}_{ij} \right)\,. 
\end{align}

For a wide-sense stationary process the mean function is independent of time, hence it can be redefined to be equal to zero without loss of generality, and the auto-covariance function is dependent only on the time lag $\tau \equiv t_i - t_j$. One can thus choose a sampling rate $\Delta t$ so that $C_{ij} = C((i-j)\Delta t)$. The autocovariance matrix thus becomes a Toeplitz matrix\footnote{
We remind the reader that a Toeplix matrix is a matrix in the form:
\begin{pmatrix}
	a_0 & a_1 & a_2 & \ldots & \ldots& \ldots &a_n\\
	a_{-1} & a_0 & a_1 & \ldots & \ldots& \ldots &a_{n-1}\\
	a_{-2} & a_{-1} & a_0 & \ldots & \ldots & \ldots  &a_{n-2}\\
	\vdots & \vdots & \vdots & \vdots & \vdots & \vdots &\vdots\\
	a_{-n +1} & \ldots & \ldots & \ldots& a_{-1} & a_0    &a_{1}\\
	a_{-n} & \ldots & \ldots & \ldots& a_{-2} & a_{-1} & a_0
\end{pmatrix}

}.
Toeplitz matrices are asymptotically equivalent to circulant matrices and thus diagonalized by the discrete Fourier transform base~\cite{Gray}.
%R. M. Gray, Foundations and Trends⃝R in Communications and Information Theory 2, 155 (2005), URL https://doi.org/10.1561/0100000006.
Some simple algebra shows that the time-domain multivariate Gaussian can be transformed into the equivalent frequency domain 
probability distribution:
\begin{align}\label{eq:Whittle}
p&\left((\tilde{x}_1, \ldots, \tilde{x}_n)\vert I\right) = \nonumber \\
    &\frac{1}{\left(2 \pi \det S\right)^{n / 2}}\exp\left(-\frac{1}{2}\sum_{ij}\tilde{x}_i S^{-1}_{ij} \tilde{x}_j \right)\,,
\end{align}
where the matrix $S_{ij} = S_i \delta_{ij}$ is an $n\times n$ diagonal matrix whose elements are the PSD $S(f)$ calculated at frequency $f_i$.
Many readers will recognize the familiar form of the Whittle likelihood that stands at the basis of the \emph{matched filter} method\cite{prob_information_theory}
%P. M. Woodward, D. W. Fry, and W. Higinbotham, Probability and information theory, with applications to radar; 2nd ed. (Pergamon Press, Oxford, 1964), URL http://cds.cern.ch/record/2031792.
and of gravitational waves data analysis, e.g. \cite{Finn_1992, FINDCHIRP}.
Thanks to MAXENT, the problem of defining the probability distribution describing a wide-sense stationary process is thus 
entirely reduced to the estimation of the PSD or, equivalently, the autocovariance function.

\subsection{Maximum Entropy Spectral Analysis} \label{sec:MESA}

In principle, if the autocorrelation was known exactly (i.e. at every time $\tau \in (-\infty,+\infty)$), the computation of the PSD 
would reduce to a single Fourier transform.
However, in any realistic setting, we are dealing with a finite number of samples $N$ from the process, hence such computation
is impossible.
Moreover, the error $\sigma_k$ in the estimate of the autocorrelation after $k$ steps increases as $\sigma \sim 1/\sqrt{N - k}$\footnote{
This is clearly understood: when computing the autocorrelation at order $k$, only $N-k$ examples of the product $x_t x_{t+k}$ are available and the variance of the average value goes as the inverse of the square root of the points considered.
}, so that only few values for the autocorrelation function can actually be computed reliably.
This bring us the core of the problem: how to give an estimate from partial (and noisy) knowledge of the autocorrelation function? MAXENT can guide us in this task without any a priori assumptions on the unavailable data\footnote{Indeed this is the largest difference with the most common Welch method. The latter assumes that the unknown values of the autocorrelation are $0$. Clearly, this assumption is unjustified and MAXENT is a good way to drop this assumption.}.

As in the previous examples, one needs to set up a variational problem where the entropy eq.~\eqref{eq:entropy_continuos} is maximized 
subject to some problem-specific constraints. 
In our case, they are i) the PSD estimate has to be non-negative; ii) its Fourier transform has to match the sample autocorrelation (wherever an estimate of this is available).

Before doing so, there is a technical problem to solve: the definition of entropy depends on a probability distribution, not on the PSD.
It can be shown, e.g.~\cite{AblesMESA, Bartlett}, that the variational problem can be formulated in terms of the power spectral density $S(f)$ alone by considering our signal as the result of the filtering a white noise process using a filter with transfer function $T(f)$ equal to $S(f)$\footnote{
A filter with transfer function $T(f)$ takes in input a time series $x_t$ and outputs a times series $y_t$ such that:
$$T(f) = \frac{\tilde{y}(f)}{\tilde{x}(f)}$$
where $\tilde{x}(f)$ denotes the Fourier transform of $x_t$ (and similarly for $y_t$)
}.
The difference in entropy between the input and the output time series (i.e. the entropy gain) obtained by such filter applied on white noise is:
\begin{equation}\label{eq:EntropyGain}
    \Delta H = \int_{- Ny}^{Ny}\log S(f) df\,.
\end{equation}
where $\Delta t$ is sampling rate and $Ny \equiv \frac{1}{2 \Delta t}$  is the Nyquist frequency.
Thus maximising Eq.~\eqref{eq:EntropyGain} is equivalent to maximize Eq.~\eqref{eq:entropy_continuos}.

Before maximizing the entropy gain, we need to include the evidence available as a form of mathematical constraints for the assignment of $S(f)$.
This is equivalent in imposing that the variational solution $S(f)$ for the PSD matches the empirical autocorrelation.
Let us define a realization of a stochastic process $(x_1,\ldots,x_N)$ with sample autocorrelations $\bar r_k,\,k=0,\ldots, N/2$, then the PSD must satisfy the following equation:
\begin{equation}\label{eq:MaxConstraint}
\int_{-Ny}^{Ny} S(f) e^{\imath 2 \pi f k \Delta t} df = \bar r_{k}\,.
\end{equation}

Thus, by maximizing Eq.~\eqref{eq:EntropyGain} with constraints in eq.~\eqref{eq:MaxConstraint}, we can give an estimate of the spectrum given an empirical time series.
This approach on PSD computation provides a result consistent with the empirical autocorrelation function whenever this is available and, at the same time, it does not make any assumption for the unavailable estimates for the autocorelation at large time lags.

Most importantly, the variational problem admits a closed-form analytical expression for $S(f)$.
The expression was first found by Burg~\cite{burg1975maximum}:
\begin{equation}\label{eq:MESApsd}
    S(f) = \frac{P_N \Delta t}{\left(\sum_{s=0}^N a_s z^s\right)\left(\sum_{s = 0}^N a^*_s z^{-s}\right)}, 
\end{equation}
where $\Delta t$ is the sampling interval of the time series, $z=\exp{(2\pi i f\Delta t)}$, $a_0$ = 1.
The vector obtained as $(1, a_1, \dots, a_N)$ is also known as the \textit{prediction error filter}.
The coefficients $P_N$ and $a_s (s > 0)$ are to be determined by an iterative process (called Burg's algorithm). The details of the derivation and the actual form for the coefficients $a_s$ can be found in appendix \ref{sec:appendix}.

\subsection{Autoregressive Process Analogy} \label{sec:autoregr}

The application of MESA is not limited to spectral estimates, but it also provides a link between spectral analysis and the study
of autoregressive processes (AR)~\cite{doi:10.1029/RG013i001p00183}.
An autoregressive stationary process of order $p$, AR($p$), is a time series whose values satisfy the following expression: 
\begin{equation} \label{eq:AR_p}
    x_t - b_1 x_{t-1} - b_2 x_{t-2} \dots b_p x_{t - p} = \nu_t
\end{equation}
where $b_1, \ldots, b_p$ are real coefficients and $\nu_t$ is white noise with a given variance $\sigma^2$.
Thus, an AR($p$) process models the dependence of the value of the process at time $t$ on every past $p$ observations, 
thus being potentially able to model complex autocorrelation structures within observations.

Thanks to Wold's theorem~\cite{Wold_theorem}, every stationary time series can be represented as an autoregressive process: this ensures that maximum entropy estimation is faithful and general; it turns out that the maximums entropy principle provides a representation of the time series as an $AR(p)$ process ad Burg's algorithm computes the autoregressive coefficients that are suitable to the available data.

To show the analogy, we compute the PSD $S_{AR(p)}$ of an $AR(p)$ process and we show that it is formally equivalent to the PSD obtained in Eq.~\eqref{eq:MESApsd}. This will also provide a direct expression for the autoregressive coefficients $b_i$ and for the noise variance $\sigma^2$.
We start taking the $z$ transform~\footnote{The $z$ transform is the discrete-time equivalent of the Laplace transform, thus taking a discrete time-series and returning a complex frequency series.} of Eq.~\eqref{eq:AR_p}: 
\begin{align}
    \sum_t x_t z^t - \sum_i b_i z^i\sum_t x_{t - i} z^{t - i} = \sum_t \nu_t z^t.
\end{align}
Calling $\tilde x(z)$ and $\tilde \nu (z)$, the transformed quantities, 
in the $z$ domain, the process takes the form:
\begin{equation}
    \tilde x(z) = \frac{\tilde\nu(z)}{\left(1 - \sum_{i = n}^p b_n z^n \right)}
\end{equation}
Since we assumed a wide-sense stationary process, $\tilde{x}(z)$ is analytic both on and inside the unit circle. Taking its square value and evaluating it on the unit circle $z = e^{-\imath 2 \pi f \Delta t}$, from the definition of spectral density one obtains:
\begin{equation}\label{eq:ARspectrum}
    S_{AR(p)}(f) = \vert \tilde x(z)\vert ^ 2 = 
    \frac{\vert \tilde \nu(f) \vert ^ 2}{\left\vert 1 - \sum_{n = 1}^p b_i e^{\imath 2 \pi f n \Delta t} \right\vert ^ 2}\,.
\end{equation}
The numerator is the spectral density of white noise $\nu_t$, i.e. its (constant) variance $\sigma^2$.

Eqs.~\eqref{eq:ARspectrum} and ~\eqref{eq:MESApsd} are equivalent, if we identify $b_i = - a_i$ and $P_N \Delta t= \sigma ^ 2$.
This shows that the MAXENT estimation of the PSD models the observed times series as an AR process and provides a {\it fit} for the autoregressive coefficients.
Furthermore, as a consequence of Wold's theorem, there is the theoretical guarantee that every stationary time series can be modelled faithfully by the MAXENT.

\subsection{Forecasting} \label{sec:forecasting}
The link between MESA and AR processes is of particular interest. Given the solution to Burg's recursion to determine the $a_k$, we automatically obtain the coefficients of the equivalent AR process, hence we are able to exploit Eq.~\ref{eq:AR_p} to perform \emph{forecasting}, thus providing plausible future observations, conditioned on the observed data.
Indeed, for an AR($p$) process the conditional probability $p(x_t|x_{t-1}, \ldots , x_{t-p})$ of the observation at time $t$ with respect to the past $p$ observation has the form:
\begin{align}\label{eq:p_forecast}
	p&(x_t|x_{t-1}, \ldots , x_{t-p}) \nonumber\\
	&= \frac{1}{\sigma\sqrt{2\pi}} \exp\left[-\frac{1}{2} \left(\frac{x_t - \sum_{i = 1}^p b_i x_{t-i}}{\sigma}\right)^2\right]\,.
\end{align}
The interpretation of Eq.~(\ref{eq:p_forecast}) is straightforward: $x_t$ follows a Gaussian distribution with a fixed variance an a mean value $m_t = \sum_{i = 1}^p b_i x_{t-i}$ computed from past observations.
Eq.~(\ref{eq:p_forecast}) provides then a well defined probability framework for predicting future observations: this is a very useful feature of MESA, that does not have an equivalent in any other spectral analysis computation methods.

\section{Validation of the model}\label{sec:validation}
It is now clear that MESA provides a recursive formula for computing the coefficients $a_k$ in Eq.~(\ref{eq:MESApsd}). The number $M$ of such coefficients is
equivalent to the maximum order of the autocorrelation $\bar{r}_m$ considered. In an ideal scenario, this would be equal to the number of points the autocorrelation is computed at (equivalent to the length of data considered). However, the computation of high order coefficients of the autocorrelation is unstable and for high enough $m$, as the estimation for  $\bar{r}_m$ shows a very high variance, broadly scaling as $\sim \left(\sqrt{M - m}\right)^{-1}$.

It is then clear that the choice of the number of samples of the discrete autocorrelation to consider is important: 
on the one hand it is advisable to include as much knowledge of the autocorrelation as possible, leading to include all the known $\bar{r}_m$; on the other hand, including values of the autocorrelation that are not reliably estimated, can be counterproductive.
The order $M$ of the autocorrelation to be considered (or, equivalently, the order $M$ of the underlying autoregressive process) is the only tuning parameter of MESA and a careful balance between this two necessities must be made when applying the algorithm.

The reminder of this section is devoted to an extensive study on how to make such choice.
In Section~\ref{sec:optimizers}, we are going to define three different \textit{loss functions} to measure how well the 
algorithm is able to reproduce a known PSD.
The basic idea is to validate, as the autoregressive order considered increase, the performance of the algorithm results 
by measuring the loss function and pick, among the orders the one that yields better results.
Second, the performance of different loss functions will be assessed by performing the spectral analysis on time series with an analytical Gaussian PSD, sec.~\ref{sec:arp_validation}.
In a last subsection~\ref{sec:LIGO_validation}, the same analysis is performed on synthetic data generated from the analytical spectrum released by the LIGO-Virgo collaboration.

\subsection{Choice of the autoregressive order}\label{sec:optimizers} 

Guided from numerical experiments, an indication on the upper bound to the autoregressive order $M_{max}$ is~\cite{doi:10.1190/1.1440902}:
\begin{equation}\label{eq:MMAx}
M_{max} = 2N / \ln{(2N)}\,,
\end{equation}
where $N$ is the number of observed points in the time-series.
However, this is just a plausible upper limit on the order of the AR process $m$ and the optimal algorithm could employ fewer points.
We then need a more sophisticated method for computing the right value for $m$.
Various loss functions to assess the algorithm performance have been proposed in literature.
We summarise them below:

\begin{itemize}
\item \textbf{Final prediction Error} 
The first criterion is due to Akaike~\cite{Akaike1970StatisticalPI}. It was proposed that $m$ should be chosen as the 
length that minimizes the error when the filter is used as a predictor, the \emph{final prediction} error (FPE): 
\begin{equation}
    FPE(m) = \mathbb{E}\left[ \left((x_t - \hat x_t) ^ 2\right) \right]
\end{equation}
with $\hat{x}_t = \sum_{i = 1}^M a_i x_{t - i}$.
Minimizing FPE is equivalent to minimizing the quantity: 
\begin{equation}
    \mathcal{L}_{\rm FPE}(m) = P_{m} \frac{N + m + 1}{N - m - 1}
\end{equation}
with $P_m$ being the estimated noise variance at order $m$, see Eq.~\eqref{eq:errorFilter1}. In the $N \to \infty$ limit, 
remembering $m_{max} \sim 2N / \log(2N)$, Akaike's loss function is equivalent to the minimization of the variance $P_m$ of the white noise of the underlying $AR(p)$ model. 

\item \textbf{Criterion Autoregressive Transfer function (CAT)}
This second loss function has been proposed by Parzen and studied in detail by Banshali~\cite{bhansali1986}. 
It is based on the assumption that the observed process is an infinite order autoregressive process 
\begin{equation} 
x_t = \sum_{i = 1}^{\infty}a_{i}x_{t-i} + \nu_t
\end{equation} 
and tries to select the order $m$ as the best finite-order approximation for the observed process. Being N the number of samples it has the property that $m \to \infty$ as $N \to \infty$. Since any real-valued stochastic process can be written as a ARMA(p,q) process (Wold theorem) i.e. an $AR(\infty)$ process, this is a physically significant property. 
The loss function has the functional form: 
\begin{equation}
    \mathcal{L}_{\rm CAT}(m) = \frac{1}{N}\sum_{k = 1}^m \frac{N - k}{N P_k} - \frac{N - m}{N P_m},
\end{equation}
and the so-chosen order is found to be asymptotically unbiased for $P_N$. 

\item \textbf{Optimum Bayes Decision rule} 
The last criterion we will consider is the Optimum Bayes Decision rule (OBD)\cite{doi:10.1029/WR018i004p01097}. Let $x(t)$ be the observed 
time series for the process that has to be described as an $AR(m)$ process, with $m$ to be determined. The OBD is obtained choosing between M different hypothesis $H_i, i = 1, \dots, M$ maximizing their posterior distribution $P(H_i\vert x(t))$. Each
hypothesis is uniquely determined from both the length and the values of the filter under study. Choosing a Likelihood of the form eq.~\eqref{eq:p_forecast} and uniform priors for both the coefficients and the hypotheses, Rao has shown that choosing the minimum for $-log(P(H_i \vert x(t))$, i.e. maximizing the posterior for $H_i$, is asymptotically equivalent to the minimization of:

   \begin{align}
        \mathcal{L}_{\rm OBD}&(m) = (N - m - 2) \log(P_m) \nonumber\\
        &+ m \log(N) + \sum_{k = 0}^{m-1} \log(P_k) + \sum_{k = 1}^{m} a_k^2\,.
    \end{align}
\end{itemize}

Once a loss function is selected, the choice of the best recursion order is straightforward: we solve the Levinson recursion~\cite{doi:10.1002/sapm1946251261} until $M_{max}$, as given in Eq.~\eqref{eq:MMAx}, iterations are reached. Then, the order $m$ is selected to be the one that minimizes the specified loss function.

In a real implementation of the algorithm, computing all the recursion up to $M_{max}$ can result in a significant waste of computational power: the optimal value is often $m_{opt} << M_{max}$ and, in such cases, computing all the values of $m$ until $M_{max}$ is not useful.
In practice, we can apply an \textit{early stop} procedure: every few iterations we look for the best order of $m_{opt}$; if this value does not change for a while, we assume that a good (local) minimum of the loss function is found and the computation is stopped.

The following sections will be devoted to the study of the statistical properties of the loss functions introduced above: we need to understand which choice provides the best quality in the reproduction of some known power spectral densities. 

\subsection{Choice of the loss function: Gaussian PSD} \label{sec:arp_validation}
We test the performance of the three loss functions on a random time series generated with a known power spectral density.
In this first experiment, we take the PSD to be a Gaussian distribution with mean $\mu = 2.5 \quad units$ and standard deviation $\sigma = 0.5 \quad units$.
The time series are generated by sampling a frequency vector from ${p(\tilde{n}(f_i)) \propto \exp{\left\{-\frac{1}{2}\sum_i\frac{n_i ^2}{S(f_i)}\right\}}}$ and performing an inverse Fast Fourier Transform on the frequency series. 
For this investigation, we generate a dataset of $N=1000$ time series of $3000$ points each.

We then apply Burg's method choosing in turn each of the loss functions on the ensemble of simulated time series, 
thus obtaining an ensemble of PSD estimates. Through these ensembles, we characterize statistically the performance of each loss function.
The disagreement between the PSD $S_i(f)$ estimated from the $i$-th simulated time series and the target PSD $S(f)$ is measured via 
the \textit{frequency-averaged relative error} $r_i$:
\begin{equation}\label{eq:freq_error}
	r_i = \frac{1}{N_f}\sum_{f_j=\{0, \hdots , Ny\}} \frac{|S_i(f_j) - S(f_j)|}{S(f_j)}
\end{equation}
where $Ny$ is the Nyquist frequency of the time series and $N_f$ is the number of the discrete frequencies the PSD is evaluated at.

For each loss function, we compute the ensemble-averaged PSD (as well as the $90\%$ confidence level) and 
the \textit{ensemble-averaged relative error}:
\begin{equation}\label{eq:ensemble_averaged_error}
	r(f) = \frac{1}{N} \sum_i \frac{|S_i(f) - S(f)|}{S(f)} \;.
\end{equation}
For each loss function, Figs.~\ref{fig:FPEmean} \ref{fig:OBDmean} and \ref{fig:CATmean} show the averaged PSD, 
as well as the ensemble-averaged relative error from Eq.~\eqref{eq:ensemble_averaged_error}.
Furthermore, Fig.~\ref{fig:optcomparison} displays the relation between $r_i$ and the autoregressive 
order $m_i$ chosen for each independently drawn time series.

\begin{figure}[t]
	\centering
	\includegraphics[width = \linewidth]{Images/optimisers_comparison/normal/FPE_spectrum_estim.pdf}
	\caption{Average spectrum for $\mathcal{L}_{\rm FPE}(m)$ with 90\% confidence regions (top) and ensemble-averaged relative error eq.~\eqref{eq:ensemble_averaged_error} (bottom). The average is computed with $1000$ realization of a $3000$ points long time series.}
	\label{fig:FPEmean}
\end{figure}
\begin{figure}[t]
	\centering
	\includegraphics[width = \linewidth]{Images/optimisers_comparison/normal/OBD_spectrum_estim.pdf}
	\caption{Average spectrum for $\mathcal{L}_{\rm OBD}(m)$ loss function with 90\% confidence regions (top) and ensemble-averaged relative error eq.~\eqref{eq:ensemble_averaged_error} (bottom). The average is computed with $1000$ realization of a $3000$ points long time series.}
	\label{fig:OBDmean}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width = \linewidth]{Images/optimisers_comparison/normal/CAT_spectrum_estim.pdf}
	\caption{Average spectrum for $\mathcal{L}_{\rm CAT}(m)$ loss function with 90\% confidence regions (top) and ensemble-averaged relative error eq.~\eqref{eq:ensemble_averaged_error} (bottom). The average is computed with $1000$ realization of a $3000$ points long time series.}
	\label{fig:CATmean}
\end{figure} 

\begin{figure}
	\centering
	\includegraphics[width = \linewidth]{Images/optimisers_comparison/normal/error_length_contour.pdf}
	\caption{Frequency-averaged relative error $r$ eq.~\eqref{eq:freq_error} vs the length $m$ of autoregressive process, for each of the $1000$ independent realizations of the synthetic time series with a gaussian PSD. Different colours refers to different choices for the loss function: in red $\mathcal{L}_{\rm FPE}(m)$, in green $\mathcal{L}_{\rm OBD}(m)$ and in black $\mathcal{L}_{\rm CAT}(m)$. The top and right histograms show the marginal distributions.}
	\label{fig:optcomparison}
\end{figure}

\paragraph{Final Prediction Error (FPE)}
The results of our investigation on the performance of FPE are shown in Fig.~\ref{fig:FPEmean}.
Qualitatively, there is a good agreement between the reconstructed spectrum and the true spectrum; 
however, we note that the reconstruction is not very accurate in the low frequency region. 
Furthermore, the $90\%$ credible region is very small: this means that if we randomly take two of the reconstructed spectra, we expect their differences to be statistically small. These facts are evidence for FPE to be a reliable loss function.

By looking at Fig.~\ref{fig:optcomparison} (red series), we note that the AR orders obtained with 
FPE are clustered in a very small region. This is also a desirable property: FPE, in fact, provides a stable 
estimation of the ar order, which does not affect much the reconstruction error.
We conclude that the FPE shows good quality reconstruction for the spectrum and very desirable stability properties, its estimate for filter's length $m$ is clustered in a region where there is no dependence of error on $m$. 
\paragraph{Optimum Bayes Decision Rule (OBD)}
The second loss function we consider is OBD and the results are summarized in Fig.~\ref{fig:OBDmean}. As for the FPE case, they show a good agreement between the average over the reconstructions and the true spectrum.
Qualitatively the same behaviour of FPE is observed: a good quality reconstruction at the intermediate and high frequencies with a narrow $90\%$ confidence level as well as a degrading performance at low frequencies.
However, when looking at the error, the disagreement of this method is found to be larger compared to FPE: 
in the worst case, the error can be as large as a factor of $2$ compared to FPE. 

The error as a function of the AR length (green series in Fig.~\ref{fig:optcomparison}) clusters in a small region, indicating the stability of the reconstructed process order $m$. We note that on average, OBD tends to choose smaller values of $m$ with respect to FPE.

\paragraph{Criterion Autoregressive Transfer function (CAT)}
The performance of CAT is shown in Fig.~\ref{fig:CATmean}. At a first glance, CAT differs substantially from the other two loss functions considered.
The ensemble-average PSD matches very well the underlying ``true" PSD. This is also true even in the low frequency region, 
where both OBD and FPE showed poor performance.
However, the variance of the reconstructed spectrum is quite large (much larger than for FPE and OBD), and the relative error is quite high, $\sim 10\%$ and it is approximately constant over all the frequencies.

The reason for this behaviour becomes clear from Fig.~\ref{fig:optcomparison} (black series): CAT does not converge 
to any specific value for the order of the AR process. The estimated $m$ spans a large range of values, hence the large variance observed.
Fig.~\ref{fig:optcomparison} also shows a strong dependence of the error on the estimated length of the filter. The good quality of the reconstruction from the average spectrum can be explained as follows: long filters are able to capture features that short filters cannot see, like outliers in different realizations of the time-series, but this is also responsible of an increased variance in the estimate, by introducing spurious peaks in the reproduction.

When averaging the different PSD estimates, the noise in each spectrum cancels, as expected from the Gaussian nature of the AR process. 
This implies, in a sense, that each estimate of the spectrum is independent of any other, as suggested by the huge variance in the residuals. This lack of stability is not a good property for the estimation of the PSD from a single realization of the time-series, however, thanks to the averaging out of errors, this estimator seems optimal in the case of repeatable experiments and ensemble-averages.

\paragraph{Final remarks on the choice of the loss function}

In our analysis, the FPE and OBD loss functions are found to behave similarly while CAT shows fairly different properties.
CAT provides an accurate average spectrum over all the frequencies at the price of a large variance; in turn OBD and FPE 
provide a poorer average in the low frequency tails, however they also display a  smaller variance, with FPE showing the lowest.

The poor low-frequency reconstruction from OBD and FPE might be due to the fact that the first tends to select shorter filters, whereas long filters are required to model low frequency correlation.
This seems to be confirmed by looking at fig.~\ref{fig:optcomparison}. OBD, which select the shortest filters, can provide an error as large as $300 \%$ at the extrema, as compared with $85\%$ error of FPE. In turn, CAT is $5\%$ accurate in these regions.

However, we also note that the when inferring PSDs from a single time-series realization, FPE provide the lowest averaged error over all frequencies, while CAT can reach errors $5$ times larger (see again Fig.~\ref{fig:optcomparison}).
Hence, while CAT is the loss function that minimizes errors in the low-frequency end of the spectrum, FPE obtains the best overall accuracy. 

The conclusion is clear: even if in some cases CAT is more accurate when taking the average over several realizations of the underlying process, 
FPE guarantees that the single estimation is more faithful.
As in any common situation we cannot perform such averaging over different realization of the same time series, we must prefer FPE over CAT (let alone OBD, which even though qualitatively similar to FPE has worse performance).
However, if we indeed can measure the PSD by averaging over different time series, using CAT as a loss function is the best choice. In this sense we retain CAT to provide the best, and most similar in spirit, alternative to the commonly employed Welch estimation method whenever ensemble-averages are needed and justified.

\subsection{Choice of the loss function: LIGO Spectrum} \label{sec:LIGO_validation}
We continue our characterization of the various loss functions considered in this work, by investigating the 
reconstruction of a specific, known, power spectrum: that is the Advanced LIGO design sensitivity theoretical spectral curve\cite{LIGO_PSDs}.
For this analysis, we generate $N = 500$ time series of $40960$ points each, sampled with a sampling rate of $\SI{2048}{Hz}$, hence we fix the duration
of the time-series to $\SI{20}{s}$. The chosen length is convenient to capture fairly accurately the low-frequency features of the LIGO PSD.
We report our findings in Figs.~\ref{fig:ligospectrum}~and~\ref{fig:LigoOrderError}.

\begin{figure}
    \centering
     \includegraphics[width = \linewidth]{Images/optimisers_comparison/ligo/compare_estimates.pdf}
      \caption{Average spectrum for the three different loss function as compared with the ``true" PSD. The average is computed over $500$ realization of a $40960$ points long time series.}
       \label{fig:ligospectrum}
\end{figure}

Fig.~\ref{fig:ligospectrum} shows the simulated spectrum (dashed line) and the ensemble-averaged reconstructed PSD adopting the FPE (green line), OBD (red line) and CAT (black line).
In all cases, the spectrum is well reconstructed, but with a fairly distinct behaviour at low frequency, where CAT -- as in the Gaussian case -- better captures and resolves the distinct spectral feature at $\sim \SI{17}{Hz}$.
In Fig.~\ref{fig:ligoPeaks}, we report the reconstructed spectra around the two peaks at $\sim \SI{17}{Hz}$ and $\sim \SI{438}{Hz}$ (left and right panel respectively).

Fig.~\ref{fig:LigoOrderError} shows the distribution of recovered AR orders $m$ against the relative frequency-averaged error. The behaviour
of the three loss functions is very similar to what found in the Gaussian PSD case (compare it with fig.~\ref{fig:optcomparison}): ODB infers the smallest orders and gives average errors around $20$\%, FPE consistently estimates orders of a few hundreds and shows the smallest errors $\sim 15$ \% while CAT does not show any preference towards any AR order and displays wildly varying errors. Yet again, when the PSD is averaged over multiple realization of the time, CAT is able to capture the spectrum very precisely. In fact, even in presence of very sharp spectral features, CAT reconstruction seems to be almost perfectly coincident with each of them.
Hence, also the study of simulated LIGO data seems to indicate that whenever and wherever ensemble-averaged PSD are necessary, CAT is the optimal choice of loss function. However, on a single time-series realization, FPE is the more robust choice.

Let us summarize some key general conclusions:
\begin{itemize}
	\item there are no reasons to prefer OBD over CAT or FPE;
	\item if we have one single realization for the process, we recommend the use of FPE, that would get the best resolution possible. In this situation, CAT would provide spurious and unreliable results, with large error;
	\item in the case of several realizations of the same process, CAT ensemble-average properties provide very a precise spectral estimation.\end{itemize}

Therefore, the choice of loss function, at least in between CAT and FPE, depends on the problem one is attempting to solve.
\begin{figure}
    \centering
    \includegraphics[width = \linewidth]{Images/optimisers_comparison/ligo/error_length_contour.pdf}
    \caption{For each of the $500$ independent realization of the time series, we plot the relative error $r$ (as in eq.~\eqref{eq:freq_error}) against the length $m$ of autoregressive process. The time series are randomly drawn with a the analytical LIGO PSD in fig.`\ref{fig:ligospectrum}. Different colors refers to different choices for the loss function. Histograms for the distribution od the individual quantities are also represented.}
    \label{fig:LigoOrderError}
\end{figure}

\begin{figure}[t]
        \includegraphics[width = \linewidth]{Images/optimisers_comparison/ligo/compare_estimates_peaks.pdf}
        \caption{Details of peaks of the spectrum and their reconstruction with every optimizer}
        \label{fig:ligoPeaks}
\end{figure}

\subsection{How well is the AR order recovered?}

\begin{figure}
	\centering
	\includegraphics[width = \linewidth]{Images/arp_errors/scatter_deltap_ptrue}
	\caption{Reconstructed value for the autoregressive order plotted against the true value of the autoregressive order.
	The reconstructed autoregressive orders are computed from a time series randomly drawn with an $AR(p)$ model, with the three different loss functions under investigation.
	}
	\label{fig:p_vs_ptrue}
\end{figure}

We now address the issue of how well the AR order (i.e. the number of $a_k$ coefficients employed) is estimated by each loss function.
In doing so, we generate $100$ autoregressive processes $AR(p)$ with a random value of $p$, drawn such that $log(p) \sim\mathcal{U}_{[log(2), log(5000)]}$.
Each coefficient $a_k$ is assigned according to a Dirichlet distribution $\mathrm{Dir}([1,\hdots, 1])$. The sign of $a_k$ is assigned randomly according to a binomial distribution. We report the result of this investigation in Fig.~\ref{fig:p_vs_ptrue}.

It can be seen that OBD and FPE loss functions show very similar behaviour. They are {\it very} reliable in capturing the correct AR order up to a certain threshold ($p\sim 100$ for OBD and $p\sim 200$ for FPE). For any AR process of order higher than the threshold, the optimization underestimates badly the actual value of $p$.
For this reason, FPE and OBD seem reliable only for relatively simple AR processes. For more complicated processes, they seem to ``underfit'' the problem (i.e. they output a model simpler that those required).

On the other hand, CAT shows a different behaviour. The selected AR order $p$ is always close to the maximum possible value, regardless the actual value $p_{true}$ of the underlying process. This produces always a model that is more complex than those obtained with FPE and OBD and this can explain the origin of the high variance in the estimation observed in the experiments described in the previous sections.
Of course, CAT does not perform a good job in reconstructing the AR process. However, for high order AR processes, the error introduced by CAT is more tolerable than the error introduced by FPE and OBD. This is a good reason to prefer CAT in these situations (see also Sec.~\ref{sec:temp_timeseries} for another example of this effect).

\section{Comparison with Welch method}\label{sec:Welch_comparison}
In this section we perform a qualitative comparison between the performance of the MESA and of the standard Welch algorithm.
Although similar studies can be drawn from any other PSD, for definiteness, we focus on the analytical PSD computed for LIGO Handford interferometer, released together with the GWCT-1 catalog \cite{GWTC1}.

We simulate data\footnote{This is to ensure that we have a baseline PSD to compare the data with} from the PSD around the event GW150914 and we employ both Welch's method and MESA to estimate the spectrum.
We vary the length of the data used for the estimation: this is also useful to assess how the computation depends on the data available. We set the total observation time $T = 1, 5, 10, 100, 1000 \SI{}{s}$
For the MESA algorithm, we choose the FPE loss function. For the Welch algorithm, we employ a Tukey window with a parameter $\alpha$ 
equal to 0.4, an overlap fraction of $1/2$ for the segments and a length of 
segments $L = 512, 1024, 2048, 8192, 32768$ points, depending on the observation time.
In all cases, the sampling rate is set to $\SI{4096}{Hz}$.
For the Welch algorithm, we use the standard implementation provided by the python library \texttt{scipy}~\cite{numpy,scipy}.
The results from both methods are summarized in  Figs.~\ref{fig:MESA_LIGO_data} and~\ref{fig:welch_LIGO_data} respectively.

First of all, we note that using a longer time series results in a better estimation of the PSD, especially at low frequencies. 
This is somehow obvious: longer data streams probe lower frequencies thanks to Nyquist's theorem as well as providing better estimates for the FFT, in the Welch case, and the sample autocorrelation, for MESA.

We also note that MESA converges to the underlying spectrum much faster than Welch's method, providing a better estimate even in the case of short time series. Although observed at every frequency, this behaviour is more evident in the low frequency region. An accurate profile reconstruction can be obtained with MESA using a 5 seconds-strain only, while Welch method requires at least 10 seconds of data to obtain a comparable profile.
Furthermore, MESA is able to model all the details of the peak at around $\sim \SI{40}{Hz}$ (even with $T=\SI{100}{s}$), while the Welch's algorithm fails to do so even with an observation time of $T=\SI{1000}{s}$.

Another important element is the noise of the spectral estimation: we find that the PSD estimation provided by the Welch's method is more noisy (i.e. has a large number of spurious peaks) compared to the PSD measured with MESA and FPE loss function. This is especially true at high frequencies and for long observation times $T$.

Finally, as already discussed Welch's method is very dependent on the choice of window function. 
Our choice, a Tukey window with aforementioned parameters, is the best compromise with found between noise 
and accuracy for the reconstruction, but different choices can be made, possibly providing more accurate results than the ones reported here. 
However, we want to stress that this fact does not invalidate our discussion but reinforces it: one of the most appealing advantages of MESA is the minimal amount of fine tuning required.

\begin{figure*}

\begin{minipage}{0.99\columnwidth}
	\caption{Comparison between analytic (dashed line) and estimated (red line) spectrum. The estimation is performed with Maximum Entropy method on \textit{synthetic} data, with an increasing observation time $T = 1, 5, 10, 100, \SI{1000}{s}$.}
	\label{fig:MESA_LIGO_data}
	\includegraphics{Images/comparison_LVC_data/comparison_LVC_data_MESA.pdf}
\end{minipage}\hfill
\begin{minipage}{0.99\columnwidth}
	\caption{Comparison between analytic (dashed line) and estimated (green line) spectrum. The estimation is performed with Welch's method on \textit{synthetic} data with an increasing observation time $T = 1, 5, 10, 100, \SI{1000}{s}$.}
	\label{fig:welch_LIGO_data}
	\includegraphics{Images/comparison_LVC_data/comparison_LVC_data_Welch.pdf}
\end{minipage}

\end{figure*}



\section{Applications} \label{sec:applications}
\subsection{Temperature Time Series} \label{sec:temp_timeseries}
As a further example of the breadth of applicability of MESA, we applied our implementation to atmospheric temperature time series. 
The reason for this choice is twofold: i) atmospheric temperature time series present a variety of overlapping periodicities most of which are 
known; ii) it provides a stress test for the time series forecast analysis.  As dataset, we  used the historical reanalysis data 
from the ``ERA5-Land hourly data from 1981 to present" dataset, downloaded from the Climate Data Store~\cite{ERA5_dataset}.
The data consist in temperatures taken at coordinates $\textrm{N}\;\ang{45;5;} \;\; \textrm{E}\;\ang{9;1;}}$, corresponding about to the city of Milan, Italy. The temperatures are given on an hourly cadence for almost 31 years from 31st December 1989 to 30th November 2020.

Fig.~\ref{fig:temp_spectrum}, shows the MESA spectrum inferred from the data. As a comparison, we adopt both the FPE and the CAT loss functions.
In agreement with what we found in previous sections, the FPE PSD is more regular compared to the one from CAT. Both spectra show a
peak at the frequency $f_D = \SI{1}{day^{-1}}$ corresponding to one day, corresponding to the day-night cycle: this is expected.
Higher order harmonics (corresponding to integer multiples of $f_D$) are also visible up to the Nyquist frequency ($f_D = \SI{0.5}{hour^{-1}}$): they corresponds to signals that preserve the 1 day period while, at the same time, capturing the complex variability of the daily temperature cycle throughout the year.

Looking at low frequencies, FPE does not capture the yearly variability at $f_{yr} = \SI{1}{yr^{-1}}$. On the other hand, the peak is captured well by the CAT loss function. In analogy as what observed for the peak at $f_D$, in the CAT spectrum we observe a higher order harmonics at a frequency $2 \cdot f_{yr}$. As in the previous case, this is required to better model the temperature variation in the year.

The failure of FPE in capturing the low frequency trend can be understood by looking at Fig.~\ref{fig:p_vs_ptrue}. It is shown that CAT systematically select very long filters (i.e. large $p_{CAT}$ values for the autoregressive orders), whereas FPE tends to select smaller values for the order $p_{FPE}$ of the autoregressive process, often underestimating the actual value.
In this example, we have $p_{CAT} \sim  36500 \sim  \SI{4}{yr}$, whereas $p_{FPE} \sim  1300 \sim  \SI{2}{months}$.
The autoregressive process selected by FPE, hence cannot produce a meaningful prediction on the timescale longer than a few weeks and for this reason it is unable to capture the low frequency peak at $f_{yr}$. In other words, the model is too simple to model \textit{both} the behaviour at high and low frequencies (separated by approximately 3 orders of magnitude).
On the other hand, the model chosen by CAT is much more complex and is able to model also the high frequency behaviour, at the expense of making a more noisy estimation.
The ``actual" length of the autoregressive process should be closer to the choice of CAT.

We now assess the accuracy of the forecasting of new observations of the time series. 
Based on actual data, we try to predict future values as described in sec.~\ref{sec:forecasting} (of course the MESA is performed with CAT loss function). We produce $N =100$ independent predictions and we compute the median as well as the $90\%$ confidence interval. This is compared with the actual measured temperature values. The predictions spans a two years range of time. We report our results in Fig.~\ref{fig:temp_spectrum}.

We note the observed difference is always well included in the $90\%$ confidence interval: the forecasting predictions seem reliable.
On the other hand, the confidence interval is pretty large (almost as large as $\SI{15}{K}$), thus making ``easy" for the actual data to fit the predictions.
Indeed, the prediction model, while suitable for spectral estimation, is nothing more than a linear regression (plus noise term). Such a simple model hardly catches the complex trend in the variability of the temperature daily trend during a year. For more precise predictions, probably one should consider nonlinear regression model, tapping into the wealth of non linear predictors offered by the field of Deep Learning.

\begin{figure}
	\caption{Spectrum for the historical temperature time series. The two lines refers to different loss functions (CAT and FPE); the inset shows the harmonics of the fundamental frequency of a day. Two vertical lines are drawn in correspondence to the frequency $f_D$ of a day and $f_{yr}$ of a year.}
	\label{fig:temp_spectrum}
	\includegraphics{Images/climate_plots/temp_spectrum.pdf}
\end{figure}

\begin{figure}
	\caption{Difference between the historical temperature $T$ and the forecast temperature $T_{forecast}$ for two years of data. The red shadow denotes the $90\%$ confidence interval of the predictions. The model is trained with CAT loss function. The discrepancy between predicted and actual values is always everywhere included in the $90\%$ confidence interval.}
	\label{fig:temp_forecast}
	\includegraphics{Images/climate_plots/forecast_accuracy.pdf}
\end{figure}

\subsection{Forecasting the LIGO strain} \label{sec:LIGO_forecasting}
As a last example of an application of MESA, we return to the study of the strain produced by the LIGO-Virgo interferometers 
and we forecast the future observations. We focus on the public data released by the LIGO/Virgo collaboration~\cite{Abbott_2021}.
We reconstruct the PSD both assuming the CAT and FPE loss functions on $\SI{1000}{s}$ of data \cite{Abbott_2021} from the Livingston observatory
starting from GPS time $1164603392$. The data are sampled at $\SI{4096}{Hz}$.

We then forecast the following $\SI{100}{s}$ of observations with the model optimized with CAT. The results are shown in fig.~\ref{fig:LIGO_forecast}.
The prediction is always in the $90\%$ confidence interval. The confidence interval, however, is very broad and the prediction is not very accurate (being the same order of magnitude of the strain).
By looking at the standard deviation $\sigma$ of the predictions (bottom panel in Fig.~\ref{fig:LIGO_forecast}), we note that it increases very quickly in the first $\SI{0.5}{s}$.
The order of the autoregressive process selected is $p_{CAT} = 57766 \sim \SI{14}{s}$, which is much larger than the region in which $\sigma$ is small $\sim \SI{0.2}{s}$. The implication is that the series is very difficult to predict: the knowledge of past observations is not very helpful to predict future observations. Although FPE selects an autoregressive order $p_{FPE} = 29924 \sim \SI{7}{s}$ smaller than CAT,  the forecasted time series behaves very similarly.

Despite poor predictions on long timescales, the AR process obtained with MESA still can be useful on short timescales. Indeed, a precise prediction of the strain time series can be beneficial in the detection of anomalies in the data and, eventually, their removal. The predictions can form an expected baseline for the strain; any anomaly (i.e. a glitch or even a transient of physical origin) can show up as a large departure from expected trend.
Moreover, if a glitch is detected, its shape can be estimated (as well as the confidence level) by subtracting the expected signal with the actual signal.
This can work: a typical glitch~\cite{Zevin_2017} can last as long as $\SI{0.2}{s}$, close to the time scale over which the forecasting is reliable.

As noted above, for pushing further the prediction performance from our simple linear predictor, more sophisticated forecasting methods are available in the Machine Learning literature (see e.g. \cite{LSTM_article, oord2016wavenet}) and they can be trained for precision forecasting. However, a Maximum-Entropy trained AR process can be a simple baseline for comparing such more advanced methods and might suffice for many purposes.
This opens a promising path in GW data analysis and other fields of physics might also take advantage from this.

\begin{figure}
	\caption{In the top panel, we show the difference between the forecasted strain values (CAT model) and the actual values observed at Livingston interferometer (blue). The red shadow denotes the $90\%$ confidence interval subtracted with the actual time series values. The confidence interval is computed on  $100$ independent random realization of the predictions.
	The bottom panel displays the standard deviation of the predictions with both CAT and FPE models. The inset shows a detail of the first $\SI{0.5}{s}$ of plot in the top panel.
	}
	\label{fig:LIGO_forecast}
	\includegraphics{Images/forecast_LIGO/forecast_accuracy.pdf}
\end{figure}


\section{Final remarks and future prospects} \label{sec:conclusion}
We presented a case study of the application of Maximum Entropy principle to the realm of spectral estimation. Albeit the methodology hereby presented is grounded on solid theoretical foundations and its merits are widely recognized, Maximum Entropy methods have yet to be adopted routinely in the study of problems related to time series. The superior nature of maximum entropy methods, and in particular of Burg's method, is exemplified by the closed form estimate of the power spectral density and by the theoretical bridge between spectral analysis and AR processes. Moreover, the method presents, in our view, two main advantages when compared with more traditional ones; first there is no need to choose an arbitrary window function to correct the data and, second it provides as straightforward way to compute predictions given past observations. Accompanying this work, we provide a publicly available \texttt{Python} implementation, called \texttt{memspectrum}, that we used to perform the numerical studies presented in this work.. 

Since the order of the AR process is not yet determined by the theory, we opted for an in-depth investigation of several proposals in the literature and found that different loss functions are required for different situations, with the FPE loss function being the most indicated to deal with gravitational wave data. Along these lines, we directly compared the PSDs computed with MESA with the canonical Welch's algorithm. As outlined in Sec.~\ref{sec:Welch_comparison}, MESA provides PSD estimates with smaller variance and better accuracy than Welch algorithm. The use of MESA is particularly useful for short time series samples, where Welch's method is outperformed in both precision and confidence.
 
This observation suggests a promising avenue to pursue in future developments of gravitational waves data analysis: for short time series, comparable with the length of binary black hole systems as observed by LIGO, Virgo and KAGRA, the computational cost of MESA is moderate and the inferred PSD is an accurate representation of the true underlying PSD, hence MESA could be employed for a simultaneous inference of the PSD and of the gravitational wave signal during a Bayesian parameter estimation exercise, effectively marginalizing over the detector noise PSD. Whenever the log-likelihood of the model for a time series takes the form
\begin{equation}
	\log\mathcal{L}(d_t | \theta) \propto  -\frac{1}{2} \frac{(\tilde{d}(f)- \tilde{x}(f;\theta))^2}{S(f)} - \frac{1}{2}\log\left[\int  df\,S(f)\right]
\end{equation}
where $\tilde{\hphantom{a}}$ denotes the Fourier transform and the signal model $x(t;\theta)$, dependent on some parameter $\theta$, is a prediction for a deterministic signal buried in the observed time series $d(t)$. The typical approach is to estimate the PSD offline on a large batch of data and off-source (see~\cite{lalinference}), that might not reflect the structure of noise in the time slice to analyse. MESA can offer a way out from this problem: at each evaluation of the likelihood, a new spectral analysis is performed by computing the PSD on $d_t-x_t$.
In this way, the PSD would depend also on $\theta$ and would effectively model the residuals. Preliminary studies suggest that this is possible~\cite{martini_thesis}.

Furthermore, MESA provides a simple, but robust and quite accurate, albeit for short times, predictor for the time series. This fact is remarkable and can be used in time series analysis for several purposes. As discussed in Sec.~\ref{sec:LIGO_forecasting}, an anomaly detection pipeline could be built using the forecasts of MESA: the predictions can form a baseline to compare the actual observations with. Whenever the observed data are outside the expectations, an anomaly detection can be claimed. Of course such predictions can be done with a more accurate (perhaps nonlinear model); however MESA has the advantage of being simple and fast to construct, while providing decent predictions. At the same time, several instruments present gaps in their data stream, for instance LISA is expected to show such gaps~(e.g. \cite{lisa_gaps} and references therein), MESA forecasting capabilities could be used to fill those gaps with predicted data from past observations.

In conclusion, we reiterate that MESA is a theoretically sound, computationally feasible and reliable way of studying the properties of stochastic processes and we hope that the investigations presented in this work will further stimulate developments and applications of this method.


\begin{acknowledgments}
We are grateful to S.~Shore, M.~Maugeri and C.~Rossi for useful discussions.\\
This research has made use of data, software and/or web tools obtained from the Gravitational Wave Open Science Center (\url{https://www.gw-openscience.org/}), a service of LIGO Laboratory, the LIGO Scientific Collaboration and the Virgo Collaboration. LIGO Laboratory and Advanced LIGO are funded by the United States National Science Foundation (NSF) as well as the Science and Technology Facilities Council (STFC) of the United Kingdom, the Max-Planck-Society (MPS), and the State of Niedersachsen/Germany for support of the construction of Advanced LIGO and construction and operation of the GEO600 detector. Additional support for Advanced LIGO was provided by the Australian Research Council. Virgo is funded, through the European Gravitational Observatory (EGO), by the French Centre National de Recherche Scientifique (CNRS), the Italian Istituto Nazionale di Fisica Nucleare (INFN) and the Dutch Nikhef, with contributions by institutions from Belgium, Germany, Greece, Hungary, Ireland, Japan, Monaco, Poland, Portugal, Spain.
\end{acknowledgments}

\pagebreak
\appendix
\section{Details of PSD computation} \label{sec:appendix}
\subsection{MESA solution}\label{sec:MESA_solution}
We derive the expression for the MAXENT spectral estimator following the approach proposed by Burg \cite{burg1975maximum}.
Unlike the standard approach, we do not enforce the constraints in eq.~\eqref{eq:MaxConstraint} with the standard Lagrange Multipliers approach.
We write instead the PSD $S(f)$ as the Fourier Transform of the sample autocorrelation function: 
\begin{equation}
    S(f) = \frac{1}{2 Ny}\sum_{n = -\infty}^{\infty} \bar r_n e^{- \imath 2 \pi n \Delta t},
\end{equation}
and, plugging it in the entropy gain expression eq.~\eqref{eq:EntropyGain}, we obtain:
\begin{equation}
    \Delta H = \int_{-Ny}^{Ny}  
    \log\left(\frac{1}{2 Ny}\sum_{n = -\infty}^{\infty} \bar r_n e^{-\imath 2 \pi f n \Delta t} 
    \right) df.
\end{equation}
Note that this expression already takes into account the constraints in eq.~\eqref{eq:MaxConstraint}.

We now introduce a set of coefficients $\lambda_s$, defined as the derivative of $\Delta H$ with respect to the autocorrelation function $r_s$.
Explicitly they are:
\begin{equation} \label{eq:lamdas}
      \lambda_s \coloneqq \frac{\delta H}{\delta \bar r_s} = \frac{1}{2Ny}\int_{-Ny}^{Ny} S(f)^{-1}e^{-\imath 2 \pi f s \Delta t } df
\end{equation} 
and we will show that $S(f)^{-1}$ can be written as a Fourier Expansion in terms of such coefficients. Then, the determination of the values for the $\lambda_s$ uniquely solves the problem of power-spectral density estimation.

Some properties for the coefficients can be worked out easily. First, since $S(f)$ is real, the $\lambda_s$ show the property 
\begin{equation}
	\nonumber 
	\lambda_s = \lambda_{-s}^*. 
\end{equation}
The second property is obtained considering that the autocorrelation function $r_n$ can only be computed for a finite time interval $n \in [-N, N]$ and that the PSD estimation must not depend on the unavailable values $r_n$: this is part of the constraint in eq.~\eqref{eq:MaxConstraint}
This requirement can be implemented as:  
\begin{equation}\nonumber 
    \frac{\delta H}{\delta \bar r_s} = 0 \text{ for } \vert s \vert > N,
\end{equation}
that means 
\begin{equation}
\nonumber 
\lambda_s = 0 \text{ for } \vert s \vert > N. 
\end{equation}

From eq.~\eqref{eq:lamdas} and from the properties above, is easily seen from the properties of the Fourier transform that $S(f)$ can be expressed via a Fourier Series 
\begin{equation}\label{eq:PSDconstraint}
    S(f)^{-1} = \sum_{s = -N}^N \lambda_s e^{-\imath 2 \pi f s \Delta t}.
\end{equation}
Defining $z = e^{-\imath 2 \pi f \Delta t}$ the previous Fourier expansion becomes a Laurent Polynomial in $z$: 
\begin{equation}
    \label{eq:zExp}
    S(f)^{-1} = \lambda_0 + \sum_{s = 1}^N \lambda_s z^s + \sum_{s = 1}^N \lambda^*_s z^{-s}.
\end{equation}
It is easy to show that if $z_0$ is a root for the polynomial $(z_0^*)^{-1}$ is also a root: for every root laying outside the unit circle there will be another root inside of it and vice-versa. These properties allow us to rewrite the Fourier expansion \eqref{eq:zExp} as \cite{1975STIN...7714318B}:
\begin{equation}\label{eq:MESApsd_appendix}
    S(f) = \frac{P_N \Delta t}{\left(\sum_{s=0}^N a_s z^z\right)\left(\sum_{s = 0}^N a^*_s z^{-s}\right)}
\end{equation}
with $a_0 = 1$ and $\Delta t$ the uniform sampling interval for the time series. The vector obtained as $(1, a_1, \dots, a_N)$ is the prediction error filter. The power spectral density $S(f)$ is uniquely determined if both the prediction error filter and $P_N$ coefficients are computed.

To compute the $a_s$ is convenient to plug into eq.~\eqref{eq:MaxConstraint} the Laurent Polynomial exansion for $S(f)$ eq.~\eqref{eq:MESApsd_appendix} and then integrating over $z$ (taking values on ${\mathbb S^1}$). In this way the equation becomes:
\begin{equation}
    \label{eq:contourintegral}
   \frac{P_N}{2 \pi \imath} \oint _{\mathbb S^1}\frac{z^{-s - 1}}{\sum_{n = 0}^N a_n z^n \sum_{n = 0}^N a^*_n z^{-n}}dz = \bar r_s. 
\end{equation}
Substituting $s \to s - r$, multiplying by $a^*_s$ and summing over $s$, the previous equation becomes 
\begin{equation}
    \label{eq:errorFilter}
    \sum_{s = 0}^N a_s \bar r_{s - r} = \frac{P_N}{2 \pi \imath}\oint \frac{z^{r -1}}{\sum_{s = 0}^N a_s z^s}dz
\end{equation}
For a wide-sense stationary processes, all the poles lay outside the unit circle so that the previous integral can be easily computed obtaining the following, well known, equations: 
\begin{align}
    \label{eq:errorFilter1}
    \sum_{s = 0}^N a_s \bar r_{r - s} &= P_N \quad \text{ if } r = 0 \\ \label{eq:errorFilter2}
    \sum_{s = 0}^N a_s \bar r_{r - s} & = 0 \qquad \text{ if } r \neq 0.
\end{align}

\subsection{Levinson recursion} \label{sec:LevinsonRecursion}
The solution of the eqs.~(\ref{eq:errorFilter1}-\ref{eq:errorFilter2}) fully determines the functional form of the power spectral density estimator \eqref{eq:MESApsd_appendix}.
The method for solving the equations is called the Levinson-Durbin recursion \cite{doi:10.1002/sapm1946251261} and it is described in the following.
For each order $N$ of the iteration we define the quantities:
\begin{align}
\Delta_N &= \sum_{n = 0}^{N} a_n \bar{r}_{N - n + 1} \\ 
c_N &= - \frac{\Delta_N}{P_N},
\end{align}

The Levinson recursion computes the $N$th order quantities given the $N-1$th order quantities: 
\begin{equation} \label{eq:Levinson1}
P_N = P_{N -1}\left(1 - \vert c_{N - 1} \vert ^2\right)
\end{equation}
and 
\begin{equation} \label{eq:Levinson2}
\begin{pmatrix}
1 \\ a_1 \\ \vdots \\ a_{N - 1} \\ a_N
\end{pmatrix}
= 
\begin{pmatrix}
1 \\ b_1 \\ \vdots \\ b_{N -1} \\ 0
\end{pmatrix}
+ c_{N-1}
\begin{pmatrix}
0 \\ b_{N -1}^* \\ \vdots \\ b^*_1 \\ 1
\end{pmatrix}. 
\end{equation}
where $b$ holds the value of the $a_s$ coefficients at order $N-1$. 
The 0-th order element can be easily initialized reminding that $a_0 = 1$ (always) and that $P_0$ can be determined from \eqref{eq:errorFilter1}.
Its values turns out to be: 
\begin{equation}
P_0 = R(0),
\end{equation}
$\Delta_0$ and $c_0$ are uniquely determined from their definitions and they are:
\begin{equation}
\Delta_0 = R(1); \quad c_0 = -\frac{R(1)}{R(0)}. 
\end{equation}

These expressions allow us to compute $\vec a$ and $P_N$ to any order by simply iterating \eqref{eq:Levinson1} and \eqref{eq:Levinson2}. Substituting them in equation \eqref{eq:MESApsd_appendix} the problem of the estimation for the power spectral density via maximum entropy principle is solved.
Burg's method for spectral analysis is solved via Levinson is implemented in the released \texttt{memspectrum} package.
Another faster recursion method is available in \cite{Vos} and it is also available in \texttt{memspectrum}.

\newpage

	\bibliography{Bibliography.bib}
	\bibliographystyle{ieeetr}

\end{document}








