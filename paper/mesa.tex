\documentclass[twocolumn,showpacs,preprintnumbers,nofootinbib,prd,
superscriptaddress,10pt]{revtex4-1}

\usepackage{amsmath,amssymb}
\usepackage{amsfonts}
\usepackage[normalem]{ulem}
\usepackage{textcomp}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{bm}
\usepackage{afterpage}
\usepackage{graphicx}
\usepackage{psfrag}
\usepackage{mathtools}
%\usepackage{tensor}
\usepackage{layouts}
%\usepackage{DejaVuSans}
\usepackage{epstopdf}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[utf8]{inputenc}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{tabularx}
\usepackage{ragged2e}
\usepackage{cleveref}
\usepackage{blindtext}
\usepackage{graphicx}
\usepackage{siunitx}
	\sisetup{output-decimal-marker={.}}
\newtheorem{theorem}{Theorem}
\usepackage{float}
	

	%some math symbols
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\DeclareMathOperator{\sign}{sign}
\renewcommand{\d}[1]{\ensuremath{\operatorname{d}\!{#1}}}
%argmin and argmax
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% comments command
\newcommand{\wdp}[1]{{\textcolor{magenta}{\texttt{WDP: #1}} }}
\newcommand{\amartini}[1]{{\textcolor{blue}{\texttt{AM: #1}} }}
\newcommand{\sschmidt}[1]{{\textcolor{red}{\texttt{SS: #1}} }}

\begin{document}

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ABSTRACT
\begin{abstract}
Burg method of Maximum Entropy Spectral Analysis (MESA) provides a powerful tool to perform spectral estimation of a time-series. The method is a generalization of Jaynes maximum entropy principle and provides the estimate for the spectrum in terms of the coefficients of some autoregressive process AR(p) or order $p$.
A closed form recursive solution provides an estimate of the autoregressive coefficients as well as of the order $p$ of the process.
We provide a ready to use implementation of the algorithm in the form of a python package `memspectrum`. We characterize our code performing a power spectral density analysis on some syntethic generated data (with known power spectral density) and we compare different criteria for stopping the recursion. Furthermore, we compare the performance of our code with the traditional Welch algorithm, using both syntethic data and real gravitational waves data from the LIGO-Virgo collaboration.
We find that, when compared to the Welch's method, the Burg's method provides a power spectral density (PSD) estimation with a systematically lower variance. This is particularly manifest in the case of a little number of datapoints, making Burg method most suitable to work in this regime.
%Several methods are characterized on synthetic noise having a known power spectral density (PSD) via the computation of the percentage error and then compared.  

\end{abstract}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TITLE
	\title{Maximum Entropy Spectral Analysis: a case study}
	\author{Alessandro \surname{Martini}}
		\email{martini.alessandr@gmail.com}
        \affiliation{Dipartimento di Fisica  Università di Pisa, and INFN Sezione di Pisa, Pisa I-56127,Italy}        
	\author{Stefano \surname{Schmidt}}
		\email{s.schmidt@uu.nl}
        \affiliation{add me}      
	\author{Walter \surname{Del Pozzo}}
		\email{walter.delpozzo@unipi.it}
        \affiliation{Dipartimento di Fisica  Università di Pisa, and INFN Sezione di Pisa, Pisa I-56127,Italy}      
  
	
	\maketitle
	\tableofcontents

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BODY  

\section{Introduction}

The study of the properties of  stochastic processes is a crucial task in many fields of physics and mathematical physics [Here we should make some examples...].
A particularly interesting class of stochastic processes are the so-called \textit{wide-sense} stationary processes. These are stochastic processes that display an invariance of their statistical properties, such as their two-point autocovariance function, with respect to time translation. If $x(t)$ is a wide-sense stationary process, it is completely determined by the knowledge of the autocorrelation function 
\begin{equation}
	C(\tau) = \mathbf{E}[x_t \cdot x_{t+\tau}]
\end{equation}
or equivalently by the knowledge of their \emph{power spectral density} (PSD) $S(f)$. Thanks to the Wiener-Khinchin theorem the two are related by a Fourier transform: 
\begin{equation}\label{eq:psd-autocorrelation}
	S(f) = \int_{-\infty}^{\infty} \textrm{d}\tau C(\tau) e^{-i 2 \pi f \tau}\,.
\end{equation}
In some literature, the PSD is introduced as  
\begin{equation}\label{eq:psd-f-definition}
	\mathbf{E}[\tilde{x}(f) \cdot \tilde{x}(f^\prime)] = S(f) \delta(f-f^\prime)
\end{equation}
without highlighting its connection with the time structure of the process itself, thus masking some important properties that will explore further in what follows. The latter definition in~\eqref{eq:psd-f-definition} gives, however, i) a straightforward interpretation of the PSD: it measures how much signal ``power" is located in each frequency; ii) an operative way of estimating it for an unknown process.  
%Under such assumption, it can be shown [cite something] that the problem of characterizing the noise properties is greatly simplified and its properties are summarized by a single \textit{autocorrelation function}, which describes how an observation at a given time correlates with another observation a later time. More formally, the properties of a zero mean time series $x_t$ are described by the function:
%\begin{equation}
%	R(\tau) = \mathbf{E}[x_t \cdot x_{t+\tau}]
%\end{equation}
%where the average is computed on different times $t$.
%Thus, a stationary noise time series is completely characterized by the autocorellation function.
%\par
%More often, the study is performed in frequency domain: indeed, for stationary noise the components in frequency $\tilde{x}(f)$ of a stationary time series are conditionally independent from each other. It can be shown that:
%\begin{equation}
%	\mathbf{E}[\tilde{x}(f) \cdot \tilde{x}(f^\prime)] = S(f) \delta(f-f^\prime)
%\end{equation}
%This equation defines the function \textit{power spectral density} (PSD) $S(f)$.
%The PSD has a straightforward interpretation: it measures how much signal ``power" is located in each frequency bin. Furthermore, it is not difficult to show that it is the Fourier transform of the autocorrelation function:
%\begin{equation}
%	S(f) = \int_{-\infty}^{\infty} \textrm{d}\tau R(\tau) e^{-i 2 \pi f \tau}
%\end{equation}
%A power spectral density fully describes a stationary noisy time series and this shows how important is in the study of noise.
%\par
%Thus, in many real application, the knowledge of the PSD is crucial for the understanding of the process and indeed algorithms for PSD computation are a standard part of every software package for computational physics.

An ubiquitous method for such computation is due to Welch \cite{Welch1967} and it based on eqs.(\ref{eq:psd-autocorrelation}-\ref{eq:psd-f-definition}).
Basically, the PSD is obtained by slicing the observed realisation $x(t_1),\ldots,x(t_n)$ of the process $x(t)$ into many windowed batches and averaging the squared moduli of their Fourier transforms.
This approach is equivalent \cite{Lomb,Scargle} to taking the Fourier Transform of the sample autocorrelation $\rho$, written as
\begin{equation}
    \rho_{Windowed} = \left\{W_0\rho_0,W_{\pm 1}\rho_{\pm 1}, \dots, W_{\pm M}\rho_{\pm M}, 0, 0, \dots \right\},
\end{equation}
where $M$ is the maximum time lag at which the autocorrelation is computed.
The sequence $W$ is a window function that can be chosen in several different way, each choice presenting advantages and disadvantages for the final estimate of the PSD.

The choice of a window function is arbitrary and typically is made by trial and error, until a satisfactory compromise between variance and resolution of the estimate of PSD is reached. A high frequency resolution implies high variance and vice-versa.
Besides the window function, Welch method requires a number of arbitrary choices to be made, as the number of time slices and the overlap between consecutive slices. All the knobs must be tuned by hand and their choice can dramatically affect the PSD estimation.
The final PSD estimation depends on all such choices, hence begging the question of what the ``real" PSD is.

Another problem with this approach is the requirement for the window to be $0$ outside the interval in which the autocorrelation is computed.
We are arbitrarily assuming $\rho_j = 0$ for $j > M$ and modifying the estimate (i.e. the data) if a non-rectangular window is chosen.
Making assumptions on unobserved data and modifying the ones we have at our disposal introduces a piece of ``spurious" information about the process that we are not allowed to use.

An appealing alternative approach, based on the Maximum Entropy principle \cite{JaynesArticle,jaynes2003ptl}, has been put forward by Burg \cite{burg1975maximum}. Being rooted on solid theoretical foundations, we will see that Burg's method, unlike Welch's, does not require any preprocessing of the data and requires very little tuning of the arbitrary parameters, as it provides an iterative closed form expression for the spectrum of a stochastic stationary time series. Furthermore, it embeds the PSD estimation problem into an elegant theoretical framework and makes minimal assumptions on the nature of the data.
Lastly and most importantly, it provides a robust link between spectral density estimation and the field of autoregressive processes. This provide a natural and simple machinery to forecast a time series, thus predicting past observations based on previous ones.

In this paper we discuss the details of the Maximum entropy principle, its application to Power Spectral Density estimation with Burg algorithm and the link between Burg's algorithm and autoregressive process.
Our goal is to bring (again) to public attention Maximum Entropy Spectral analysis, in the hope that it will be widely employed as a way out to the many problem posed by the ubiquitous Welch algorithm (or other similar methods).
To facilitate this goal, we present and describe a new code, \texttt{memspectrum}, that provides a robust python implementation of the algorithm. The code is ready to use and available at the PyPI repository\footnote{
Available at link: \url{https://pypi.org/project/memspectrum/}.
}
We provide a thorough assessment of the performance of our code and we validate our results performing a number of tests. We also compare our results with those of spectral analysis carried out with the standard Welch method.
In order to apply our model on a realistic setting, we analyse some time series of broad interest in the scientific community.

Our paper is organized as follows: blablabla...


\section{Theoretical foundations}
The Maximum Entropy principle (MAXENT) is among the most important results in Bayesian probability theory. It provides a way to uniquely assign probabilities to a phenomenon in a way that best represent our state of knowledge, while at the same time it is non committal with unavailable information. Its domain of application turned out to be wider than expected. In fact, thanks to Burg \cite{burg1975maximum}, this method has also been applied to perform high quality computation of power spectral densities of time series.

After a short introduction to Jayne's MAXENT (sec.~\ref{sec:MAXENT}), we will develop in detail Burg's technique of Maximum Entropy Spectral Analysis (MESA) and show that the estimate can be expressed in an analytical closed form (sec.~\ref{sec:MESA}).
Next, we will discuss an interesting link between Burg's method and autoregressive processes (sec.~\ref{sec:autoregr}) and in sec.~\ref{sec:forecasting} we will use such link for straightforwardly forecasting a time series.

\subsection{Maximum Entropy Principle} \label{sec:MAXENT}

\sschmidt{This section might be simplified...}

Before introducing MAXENT as a variational problem, we will workout some simple examples to contexualise how information's entropy
and `uncertainty' are related. First of all,  it is worth noting that the word `information' will not be used to indicate our knowledge on the system under study: this is what we will call `evidence'. Information will be intended as in communication theory: it specifies the length of the message necessary to provide
a full description of the system under study.

For instance, consider a perfectly sinusoidal signal: knowledge of amplitude, frequency and phase are sufficient to fully reproduce it: 
it has a low information entropy content. Even less information is required if we are studying a system whose outcome is 
certain (has probability $p = 1$), as in this case, a communication is not even needed.  
Shannon \cite{Shannon} proposed the quantity
\begin{equation}\label{eq:information}
    I = \log_2 \frac{1}{p(x)}
\end{equation}
to represent the quantity of information brought by an outcome $x$ with probability $p(x)$. It is additive quantity as well as monotonically decreasing as a function of $p \in [0, 1]$. The more uncertain the outcome, the higher the information it brings.

We now seek to generalize the measure for the information in the case where different outcomes are possible.
To do so, let us consider an experiment with two different random outcomes $E_1, E_2$, with given probabilities $P_1$ and $P_2$, and we ask ourselfs which are the probability assignments that makes the outcome more uncertain (i.e. maximize the information).
If $P_1$ and $P_2$ are largely different, for istance $P_1 = 0.999$ and $P_2 = 0.001$, we clearly have enough evidence to believe that event $E_1$ will occur almost certainly, considering $E_2$ to be a very implausible outcome. The information content will be very low.
On the other hand, most unpredictable experiment happens when 
\begin{equation}\nonumber
    P_1 = P_2 = \frac{1}{2}:
\end{equation}
this describes a situation of `maximum ignorance'; we have no evidence at all and we have no way of predicting what the outcome of such an experiment will be.  The information content of this system must be high.

Of course, when assigning probabilities for a set of events, all the evidence available must be taken into account. On the other hand, the probability assignment must not introduce additional evidence in the system and thus the need to be ``the most uncertain possible given the knoledge of the system".
 
%In this context, the role played by evidence should be clear: it dictates the probability assignment in order to make the result `more certain' or `less informative'. Any assignment different from the `most uncertain' is violating the principle of it to being non committal to non-available evidence. We can only assign a more certain probability, i.e. less informative, if we have evidence for it. If we have none, the result has to be the most unpredictable, i.e. the most informative. From the point of view of communication theory: the higher the evidence on a system, the lower the information I need to send to describe it.
\par
MAXENT puts the aforementioned principle into a more precise mathematical context by stating that probabilities should be assigned by maximizing uncertainty using evidence as constraint. 
This defines a variational problem, where a numerical measure of uncertainty $H\left[p_1, \dots, p_N\right]$ has to maximized. $H$ has to be a `generalization' of information defined in equation \eqref{eq:information} to be something like the `expected information' brought by an experiment with $N$ possible outcomes each with its own probability $p_i$.
%
%Some properties for $H$ can be worked out easily. 
%From previous discussion, given a fixed number $N$ of possible outcomes, we want $H$ to be maximized by the choice $p_1 = \dots = p_N$.
%Consider now the opposite situation, in which probability is assigned appealing to IP and we have a different number of possible outcomes.
%Intuitively, the most unpredictable experiment is the one with larger number of outcomes: probability of "getting it wrong" increases as $\frac{N -1}{N}$. So when equal probabilities are assigned, we want
%\begin{equation}
%    \nonumber
%    H\left[\frac{1}{N}, \dots, \frac{1}{N}\right]
%\end{equation}
%to increase monotonically with N.
%
%We also want our function to be continuous in its parameters: any small change in probability has to slightly affect uncertainty. \\
%Last is composition property. Consider three events with probability $p_1, p_2, p_3$ such that $p_2 + p_3 = 1 - p_1 = q$. We require 
%\begin{equation}
%    H\left(p_1, p_2, p_3\right) = H(p_1, q) + qH\left(\frac{p_2}{q}, \frac{p_3}{q}\right),
%\end{equation}
%so that uncertainty is additive.

Shannon \cite{Shannon} showed that the only functional form satisfying continuity with respect to its parameters and additivity is:
\begin{equation}\label{eq:entropy}
    H[p_1, \dots, p_N] = - \sum_{i = 1}^N p_i\log{p_i},
\end{equation}
or, in the continuous case:
\begin{equation}
    H[p(x)] = - \int p(x)\ln p(x) dx,
\end{equation}
and $H$ is what we will call (information) entropy.

The maximisation of entropy, supplemented by evidence in the form of constraints to which the sought-for probability distribution must obey,
gives rise to several of the most common probability distributions utilised commonly by data scientists. For instance, whenever the only constraint available is the normalisation of the probability distribution, the entropy is maximised by the uniform distribution. 
If the information available is instead the expected value -- the mean -- entropy is maximised by the  exponential distribution.

Of particular importance is the case in which, in addition to the mean, also the variance is known. MAXENT leads to the Gaussian distribution. 
We retain this particularly interesting from the foundational point of view, since it provides a deeper insight into the ubiquitous Gaussian distribution.
It is not only the limit distribution provided by the central limit theorem for finite variance processes but it is also the distribution that maximizes the entropy. For this reason, appealing to MAXENT principle, it is the correct assignment if mean and covariance are the only quantities that fully define our process. In some sense, we can interpret the central limit theorem as the natural `statistical' evolution toward a configuration that maximizes entropy. 

For this work, we are particularly interested in the mutli-dimensional case. Suppose we have a vector of measurements $(x(t_1),\ldots,x(t_n)) = (x_1, \ldots, x_n)$ that we conveniently express as a single realisation of an unknown stochastic process $x(t)$ and we have information about the expectation value of the process $\mu(t)$ and on the matrix of auto-covariances $C_{ij} \equiv C(t_i,t_j)$, then the MAXENT distribution is the $n$-dimensional multivariate Gaussian distribution\cite{gregory_2005}: 
\begin{align}
    p&\left((x_1, \ldots, x_n)\vert I\right) = \nonumber \\
    &\frac{1}{\left(2 \pi \det C\right)^{k / 2}}\exp\left(-\frac{1}{2}\sum_{i,j}(x_i-\mu_i) (x_j-\mu_j)C^{-1}_{ij} \right)\,. 
\end{align}

We have already seen that for a wide-sense stationary process the mean function is independent of time, hence redefined to be equal to zero without loss of generality, and the auto-covariance function is dependent only on the time lag $\tau \equiv t_i - t_j$. One can thus choose a sampling rate $\Delta t$ so that $C_{ij} = C((i-j)\Delta t)$. The autocovariance matrix thus becomes a Toeplitz matrix\footnote{
We remind the reader that a Toeplix matrix is a matrix in the form:
\begin{pmatrix}
	a_0 & a_1 & a_2 & \ldots & \ldots& \ldots &a_n\\
	a_{-1} & a_0 & a_1 & \ldots & \ldots& \ldots &a_{n-1}\\
	a_{-2} & a_{-1} & a_0 & \ldots & \ldots & \ldots  &a_{n-2}\\
	\vdots & \vdots & \vdots & \vdots & \vdots & \vdots &\vdots\\
	a_{-n +1} & \ldots & \ldots & \ldots& a_{-1} & a_0    &a_{1}\\
	a_{-n} & \ldots & \ldots & \ldots& a_{-2} & a_{-1} & a_0
\end{pmatrix}

}.
Toeplitz matrices are asymptotically equivalent to circulant matrices and thus diagonalised by the discrete Fourier transform base\cite{Gray}.
%R. M. Gray, Foundations and Trends⃝R in Communications and Information Theory 2, 155 (2005), URL https://doi.org/10.1561/0100000006.
Some simple algebra shows that the time-domain multivariate Gaussian can be transformed into the equivalent frequency domain 
probability distribution:
\begin{align}
p&\left((\tilde{x}_1, \ldots, \tilde{x}_n)\vert I\right) = \nonumber \\
    &\frac{1}{\left(2 \pi \det S\right)^{k / 2}}\exp\left(-\frac{1}{2}\sum_{i}\tilde{x}_i^2S^{-1}_{i} \right)\,,
\end{align}
where the matrix $S = S_i \delta_{ij}$ is an $n\times n$ diagonal matrix whose elements are the PSD $S(f)$ calculated at frequency $f_i$.
Many readers will recognise the familiar form of the Whittle likelihood that stands at the basis of the \emph{matched filter} method\cite{prob_information_theory}
%P. M. Woodward, D. W. Fry, and W. Higinbotham, Probability and information theory, with applications to radar; 2nd ed. (Pergamon Press, Oxford, 1964), URL http://cds.cern.ch/record/2031792.
and of gravitational waves data analysis, e.g. \cite{Finn_1992, FINDCHIRP}.
Thanks to MAXENT, the problem of defining the probability distribution describing a wide-sense stationary process is thus 
entirely reduced to the estimation of the PSD or, equivalently, the auto-covariance function.

%
%

%This interpretation is straightforward: if we compare it with Eq.~(\eqref{eq:information}), we see that the entropy is defined as the 'average information'. The probability assignments made maximizing it with respect to $p$, is somehow requiring  the 'expected value' of uncertainty to be a maximum. Such an assignment is in agreement with our hypothesis of being `non committal' with unavailable evidence, since \eqref{eq:entropy} is all we need to assign probabilities uniquely. Evidence can be added in form of constraints in the maximization.

%It is easy to verify that this expression for entropy verifies the aforementioned properties. 
%Assigning the same value for each probability $p_i$, it is straightforward to verify $H$ increases with N: 
%\begin{equation}
%    -\sum_{i = 1}^N \frac{1}{N}\log{\frac{1}{N}} = - \log\frac{1}{N} = \log N.
%\end{equation}
%\wdp{I need to review and correct the following}
%To show how the maximization of entropy, supplemented with additional evidence, can provide a unique probability assignment, 
%we use a simple, pedagogical example. Consider the case in which we want assign a probability distribution $p(x)$ to a variable $x$ 
%for which we know mean $\mu$ and covariance $\Sigma$. Our `evidence' is then coded as the following constraints:
%\begin{align}
%\sum_{i = 1}^{N}p_i &= 1 \\
%\sum_{i = 1}^{N}x_i p_i &= \mu \\
%\sum_{i = 1}^{N}(x_i - \mu)(x_j - \mu) p_i &= \Sigma\,
%\end{align}
%where the first row states the requirement of exhaustivity.
%\wdp{i think we want the multivariate case since i) the gaussian case is also on wikipedia ii) we actually will need that  later on.}

%First of all, we want to show that, in case in which our only constraint is exhaustiveness
%$\sum_{i = 1}^{N}p_i = 1 $,
%MAXENT reduces to indifference principle as it should be. The variational problem is defined as
%\begin{equation}\label{eq:entropyNormConst}
%    \delta\left[H - \lambda\sum_{i = 1}^N (p_i - 1)\right] = 0 
%\end{equation}
%with
%\begin{equation}
%    \delta H\left[p_1, \dots, p_N\right] = \sum_{i = 1}^N\frac{\partial H}{\partial p_i}\delta p_i = - \sum_{i = 1}^N\left(\log p_i + 1\right),
%\end{equation}
%so that equation \eqref{eq:entropyNormConst} reduce to
%\begin{equation}\nonumber 
%    \sum_{i = 1}^N\left[\log p_i + \lambda + 1 \right]\delta p_i = 0\,.
%\end{equation}
%The solution of the previous equation is easily worked out requiring every term to be zero: 
%\begin{equation}\nonumber 
%    p_i = e^{-\lambda - 1}.
%\end{equation}
%Applying the normalization condition, it is immediate to show that
%\begin{equation}
%    p_i = \frac{1}{N}.
%\end{equation}
%MAXENT reduces to the IP when only the normalization constraint is used.
%
%In the second example, we will add the constraint that the distribution has a known mean value: $\int \d{x} x p(x) = \bar{x}$. For simplicity we will work in the continuous case.
%The variational problem can be rewritten as: 
%\begin{equation}
%    \int \left[\log p(x) + 1 + \lambda + \mu x \right]\delta p(x) = 0
%\end{equation}
%with constraints: 
%\begin{equation*}
%    \int \d{x} \; p(x)  = 1 \text{  and  }
%    \int \d{x} \; x p(x) = \bar x.
%\end{equation*}
%For simplicity, we consider $x$ to be a variable ranging in the interval $(-\infty,+\infty)$, but this can be easily computed for every other interval.
%
%The general solution for $p(x)$ is:
%\begin{equation}
%    p(x) = e^{-1 - \lambda - \mu x} 
%\end{equation}
%where $\lambda$ and $\mu$ are two Lagrange multipliers
%The normalization conditions implies 
%\begin{equation}\nonumber 
%   1 =  \int \d{x} \; p(x) = e^{-1-\lambda}\int e^{-\mu x} dx, 
%\end{equation}
%so that: 
%\begin{equation}\nonumber 
%    e^{-1 -\lambda} = \mu.
%\end{equation}
%The fixed average condition provides a value for $\mu$ in terms of $\bar{x}$:
%\begin{equation}
%	\mu = \frac{1}{\bar{x}}
%\end{equation}
%The solution is then an exponential distribution:
%\begin{equation}
%    p(x) = \frac{1}{\bar{x}} \exp\left(-\frac{x}{\bar{x}}\right). \nonumber 
%\end{equation}
 
%Lastly, and more interestingly, we also consider a constraint on the variance. For this reason, we set:
%\begin{align*}
%    \int \d{x} \; p(x)  = 1 \text{  and  }
%	\int \d{x} \; x p(x) = \bar{x} \\
%	\text{  and  }
%	\int \d{x} \; (x-\bar{x})^2 p(x)  = \sigma^2
%\end{align*}
%Performing the same computation as above, we obtain a well known result:
%\begin{equation}
%	p(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2}\left(\frac{x-\bar{x}}{\sigma}\right)^2}
%\end{equation}
%
%Take a vector  $\vec x(t)$ with components in $\mathbb{R}^N$
%\begin{equation*}
%    \vec x(t) = \left(x_1, \dots, x_N \right), \quad \text{ with } x_i = x(t_i),
%\end{equation*}
%We assume $\vec x$ to be time dependent and real valued because we will work with time dependent single channel signals. If we put constraints in both mean $\vec \mu(t)$ and covariance matrix $C_(t_i, t_j)$, MAXENT allow us to assign for $\vec x(t)$ a multivariate normal distribution \cite{gregory_2005}: 
%\begin{align}
%    p&\left(\vec x(t)\vert I\right) = \nonumber \\
%    &\frac{1}{\left(2 \pi \det C\right)^{k / 2}}\exp\left(-\frac{1}{2}\sum_{i,j}(x_i-\mu_i) (x_j-\mu_j)C^{-1}_{ij} \right). 
%\end{align}



\subsection{Maximum Entropy Spectral Analysis} \label{sec:MESA}

In principle, if the autocorrelation was known exactly (i.e. at every time $\tau \in (-\infty,+\infty)$), the computation of the PSD 
would reduce to a single Fourier transform.
However, in any realistic setting, we are dealing with a finite number of samples $N$ from the process, hence such computation
is impossible.
Moreover, the error $\sigma_k$ in the estimate of the autocorrelation after $k$ steps increases as $\sigma \sim 1/\sqrt{N - k}$\footnote{
This is clearly understood: when computing the autocorrelation at order $k$, only $N-k$ examples of the product $x_t x_{t+k}$ are available and the variance of the average value goes as the inverse of the square root of the points considered.
}, so that only a few values for the autocorrelation function can actually be computed reliably.
This bring us the core of the problem: how to give an estimate from partial (and noisy) knowledge of the autocorrelation function? MAXENT can guide use in this task without any a priori assumptions on the unavailable data\footnote{Indeed this is the largest difference with the most common Welch method. The latter assumes that the unknown values of the autocorrelation are $0$. Clearly, this assumption is unjustified and MAXENT is a good way to drop this assumption.}.
\par
As in the previous examples, one needs to set up a variational problem where the entropy, Eq.~\eqref{eq:entropy}, is maximized 
subject to some problem-specific constraints. 
In our case, they are i) the PSD estimate has to be non-negative; ii) its Fourier transform has to match the sample autocorrelation (wherever an estimate of this is available).
\par
Before doing so, there is a technical problem to solve: the definition of entropy (Eq.~\ref{eq:entropy}) depends on a probability distribution, 
not on the PSD. The variational problem can be formulated in terms of the power spectral density $S(f)$ alone by
considering our signal as the result of filtering a white noise process with a filter whose power response is exactly $S(f)$ \cite{AblesMESA} \sschmidt{Which form has this filter?}. 
The entropy gain obtained by filtering white noise with a sampling rate $\Delta t$ and Nyquist frequency $Ny \equiv \frac{1}{2 \Delta t}$ is:
\begin{equation}\label{eq:EntropyGain}
    \Delta H = \int_{- Ny}^{Ny}\log S(f) df\,.
\end{equation}

Before maximising the entropy gain, we need to include the evidence available as a form of mathematical constraints for the assignment of $S(f)$.
This is equivalent in imposing that the variational solution $S(f)$ for the PSD matches the empirical autocorrelation.
Let us define a realisation of a stochastic process $(x_1,\ldots,x_N)$ with sample autocorrelations $\bar r_k,\,k=0,\ldots, N/2$, then the PSD must satisfy the following equation:
\begin{equation}\label{eq:MaxConstraint}
\int_{-Ny}^{Ny} S(f) e^{\imath 2 \pi f k \Delta t} df = \bar r_{k}\,.
\end{equation}

Thus, by maximizing eq.~\eqref{eq:EntropyGain} with constraints in eq.~\eqref{eq:MaxConstraint}, we can give an estimate of the spectrum given an empirical time series.
This approach on PSD computation provide a result consistent with the epirical autocorrelation function whenever this is available and, at the same time, it does not make any assumption for the unavailable estimates for the autocorelation at large time lags.
Most importantly, the variational problem admits a closed-form analytical expression for $S(f)$.
The expression was first found by Burg~\cite{burg1975maximum}:
\begin{equation}\label{eq:MESApsd}
    S(f) = \frac{P_N \Delta t}{\left(\sum_{s=0}^N a_s z^s\right)\left(\sum_{s = 0}^N a^*_s z^{-s}\right)}, 
\end{equation}
where $\Delta t$ is the sampling interval of the time series, $z=\exp{(2\pi i f\Delta t)}$, $a_0$ = 1.
The vector obtained as $(1, a_1, \dots, a_N)$ is also known as the \textit{prediction error filter}.
The coefficients $P_N$ and $a_s (s > 0)$ are to be determined by an iterative process (called Burg's algorithm). The details of the derivation and the actual form for the coefficients $a_s$ can be found in appendix \ref{sec:MESA_solution}.

\subsection{Autoregressive Process Analogy} \label{sec:autoregr}

The application of MESA is not limited to spectral estimates, but it also provides a link between spectral analysis and the study
of autoregressive processes.\cite{doi:10.1029/RG013i001p00183}.
An autoregressive stationary process of order $p$ (AR($p$)) is a time series whose values satisfy the following expression: 
\begin{equation} \label{eq:AR_p}
    x_t - b_1 x_{t-1} - b_2 x_{t-2} \dots b_p x_{t - p} = \nu_t
\end{equation}
where $b_1, \ldots, b_p$ are real coefficients and $\nu_t$ is white noise with a given variance $\sigma^2$.
Thus, an AR($p$) process models the dependence of the value of the process at time $t$ on every past $p$ observations, 
thus being potentially able to model complex autocorrelation between data.

It turns out that the maximums entropy principle provides a representation of the time series as an $AR(p)$ process. The Burg's algorithm computes the autoregressive coefficients that are suitable to the available data.
Thanks to Wold's theorem \cite{Wold_theorem}, every stationary time series can be represented as an autoregressive process: this ensures that maximum entropy estimation is faithful and general.

To show the analogy, we compute the PSD $S_{AR(p)}$ of an $AR(p)$ process and we show that it is formally equivalent to the PSD obtained in eq.~\eqref{eq:MESApsd}. This will also provide a direct expression for the autoregressive coefficients $b_i$ and for the noise variance $\sigma^2$.
We start taking the $z$ transform~\footnote{We remind the reader that the $z$ transform is the discrete-time equivalent of the Laplace transform, thus taking a discrete time time-series and returning a frequency series.} of Eq.~\eqref{eq:AR_p}: 
\begin{align}
    \sum_t x_t z^t - \sum_i b_i z^i\sum_t x_{t - i} z^{t - i} = \sum_t \nu_t z^t.
\end{align}
Calling $\tilde x(z)$ and $\tilde \nu (z)$, the transformed quantities, 
in the $z$ domain, the process takes the form:
\begin{equation}
    \tilde x(z) = \frac{\tilde\nu(z)}{\left(1 - \sum_{i = n}^p b_n z^n \right)}
\end{equation}
Since we assumed a wide-sense stationary process, $\tilde{x}(z)$ is analytic both on and inside the unit circle. Taking its square value and evaluating it on the unit circle $z = e^{-\imath 2 \pi f \Delta t}$, from the definition of spectral density one obtains:
\begin{equation}\label{eq:ARspectrum}
    S_{AR(p)}(f) = \vert \tilde x(z)\vert ^ 2 = 
    \frac{\vert \tilde \nu(f) \vert ^ 2}{\left\vert 1 - \sum_{n = 1}^p b_i e^{\imath 2 \pi f n \Delta t} \right\vert ^ 2}\,.
\end{equation}
The numerator is the spectral density of white noise $\nu_t$, i.e. its (constant) variance $\sigma^2$.
Eq.~(\eqref{eq:ARspectrum}) and Eq.~(\eqref{eq:MESApsd}) are equivalent, if we identify $b_i = - a_i$ and $P_N \Delta t= \sigma ^ 2$.
This shows that the MAXENT estimation of the PSD also computes the coefficients of the autoregressive 
process that model the observed process.

%\sschmidt{I don't understand the next of this section...}
%Having computed the prediction error filter one can easily obtain an estimate of the $AR$ coefficients for the process just changing sign to the prediction error filter $a_i$.
%Taken a time series, MESA allows us to compute the power spectrum and to find an effective description in terms of an autoregressive process. 
%Wold's theorem ensures that this (approximate) description for the phenomenon is legitimate when the process is found to be stationary. So, MESA defines a bridge between spectral estimation and the study of autoregressive processes, an unexpected and fundamental result. \\
%Another very remarkable process for this estimate, is that the linear predictor obtained solving the error filter equation, is the best linear predictor, i.e. it minimizes error, and is equivalent to a least square fit for the coefficients. \cite{VDBos}  \\ 
%To prove that it minimizes error, we will follow the original proof provided by Burg \cite{burg1975maximum}. Consider a general linear predictor $-c_i$: 
%\begin{equation}
%    x_{t} - \sum_{i = 1}^p (-c_i)x_{t - i} = \sum_{i = 0}^p c_i x_{t - i} = e'_t, 
%\end{equation}
%Denoting with $E\left[\cdot\right]$ the expectation value of some variable, the mean square error is given by: 
%\begin{equation}
%E\left[\sum_{i = 0}^p c_i x_{t-i}\sum_{j = 0}^p c^*_j x^*_{t - j}\right] = \sum_{i,j} c^*_j E\left[x^*_{t-j}x_{t-i} \right] c_j = \sum_{i, j}c^*_j R(j - i) c_i > 0,
%\end{equation}
%which, being $R$ positive definite and defined by the multiplication of a complex number times its complex conjugate, is always positive definite. 
%Let $-\vec{b}$ be the prediction error filter obtained with MESA. Multiplying the left hand side of \ref{eq:predictionErrorFilter} for the complex conjugate of the forward prediction error, 
%\begin{equation}\nonumber 
%    \sum_{i,j}b^*_jR(j-i)b_i = P_N
%\end{equation}
%is obtained . \\ 
%The (positive) quantity  
%\begin{equation}\label{eq:linearPredictor}
%    \sum_{j = 0}^p\sum_{i = 0}^p(c^*_j - b^*_j)R(j-i)(c_i - b_i),
%\end{equation}
%using Einstein notation on repeated indices and recall that $c_0 = b_0 = 1$, can be rewritten as 
%\begin{align}
%    & c^*_j R(j - i) c_i - c^*_j R(j - i) b_i - b^*_j R(j - i) c_i + b^*_j R(j - i)  b_i =\\ \nonumber 
%    &c^*_j R(j - i) c_i - c^*_0 P_N - P_N c_0 + P_N =\\ \nonumber
%    & c^*_j R(j - i) c_i - P_N.
%\end{align}
%Moving $P_N$ on the left hand side, it becomes 
%\begin{equation}\label{eq:bestLinearPredictor}
%    \sum_{i, j}c^*_j R(j-i)c_i = \sum_{j = 0}^p\sum_{i = 0}^p(c^*_j - b^*_j)R(j-i)(c_i - b_i) + P_N.
%\end{equation}
%Being all the quantities positive definite, the left hand side is minimized by the choice $c_i = b_i$. Also, known $P_N$, the previous equation gives an estimate on the additional mean square error associated at choices that differ from the prediction error filter. 

\subsection{Forecasting} \label{sec:forecasting}
The link between MESA and autoregressive processes is of particular interest. Given the solution to the Levinson recursion to determine the $a_k$, see Appendix A, we have automatically the coefficients of the AR with equivalent PSD, hence we are able to exploit Eq.~\ref{eq:AR_p} to perform \emph{forecasting}, thus providing plausible future observations, conditioned on the observed data.
Indeed, an AR($p$) process models the conditional probability $p(x_t|x_{t-1}, \ldots , x_{t-p})$ of the observation at time $t$ with respect to the past $p$ observation:
\begin{align}\label{eq:p_forecast}
	p&(x_t|x_{t-1}, \ldots , x_{t-p}) \nonumber\\
	&= \frac{1}{\sigma\sqrt{2\pi}} \exp\left[-\frac{1}{2} \left(\frac{x_t - \sum_{i = 1}^p b_i x_{t-i}}{\sigma}\right)^2\right].
\end{align}
The interpretation of Eq.~(\ref{eq:p_forecast}) is straightforward: $x_t$ follows a Gaussian distribution with a fixed variance an a mean value $m_t = \sum_{i = 1}^p b_i x_{t-i}$ computed from past observations.
Eq.~(\ref{eq:p_forecast}) provides then a well defined probability framework for predicting future observations: this is a very useful feature of MESA, that does not have an equivalent in any other spectral analysis computation.

\section{Validation of the model}
It is now clear that MESA provides a recursive formula for computing the coefficients $a_k$ in Eq.~(\ref{eq:MESApsd}). The number $M$ of such coefficients is the same as the number of time steps considered in the computation of the autocorrelation $\bar{r}_m$. In a perfect scenario, this would be equal to the number of points the autocorrelation is computed at (equivalent to the length of data considered). However, the computation of high order coefficients of the autocorrelation is unstable and for high enough $n$, the the estimation for  $\bar{n}_m$ shows a very high variance, broadly scaling as $\sim \left(\sqrt{M - m}\right)^{-1}$.

%OR WE CAN KEEP THIS (SOMEHOW MODIFIED)
%Before being able to apply MESA, we need to address the question of the best length $M$ of the filter that provides the optimal estimate of the power spectrum. If the autocorrelation matrix was perfectly know, the answer would be simply $M = N$.
%In real cases, such a knowledge is not available and one has to estimate the autocorrelation function from the data. The accuracy for the estimate decreases as the time lag $k$ increases as $\sim \left(\sqrt{N - k}\right)^{-1}$.
%The accuracy of the estimate should then increase as the length of the autocorrelation function increases, but, from a certain point on, inaccuracy in the estimate for $R$ becomes so large that the estimate for the spectrum is negatively affected by this fact.

It is then clear that the choice of how many samples of the discrete autocorrelation to consider is important: 
on the one hand it is advisable to include as much knowledge of the autocorrelation as possible, leading to include all the known $\bar{r}_m$; on the other hand, including values of the autocorrelation that are not reliably estimated, can be counterproductive.
Indeed, the order $M$ of the autocorrelation to be considered (or equivalently the order $M$ of the underlying autoregressive process) is the only tuning parameter of MESA and a careful balance between this two necessities must be made when applying the algorithm.

The reminder of this section is devoted to an extensive study on how to make such choice.
In Section~\ref{sec:optimizers}, we are going to define three different \textit{loss functions} to measure how well the 
algorithm is able to reproduce a known PSD.
The basic idea is to validate, as the autoregressive order considered increase, the performance of the algorithm results 
by measuring the loss function and pick, among the orders the one that yields better results.
Second, the performance of different loss functions will be assessed by performing the spectral analysis on time series with given analytical PSD, Sec.~\ref{sec:arp_validation}.
In a last subsection~\ref{sec:LIGO_validation}, the same analysis is performed on real data from the LIGO-Virgo gravitational waves observatories.

\subsection{Choice of the autoregressive order}\label{sec:optimizers} 

Guided from numerical experiments, an indication on the upper bound to the autoregressive order $M_{max}$ is~\cite{doi:10.1190/1.1440902}:
\begin{equation}\label{eq:MMAx}
M_{max} = 2N / \ln{(2N)}\,,
\end{equation}
where $N$ is the number of points in the time-series.
However, this is just a plausible upper limit on the order of the AR process $m$ and the optimal algorithm could employ fewer points.
We then need a more sophisticated method for computing the right value for $m$.
Various loss functions to assess the algorithm performance have been proposed in literature.
We summarise them below:

\begin{itemize}
\item \textbf{Final prediction Error } 
The first criterion is due to Akaike~\cite{Akaike1970StatisticalPI}. He proposed that $m$ should be chosen as the 
length that minimises the error when the filter is used as a predictor, the \emph{final prediction} error (FPE): 
\begin{equation}
    FPE(m) = \mathbb{E}\left[ \left((x_t - \hat x_t) ^ 2\right) \right]
\end{equation}
with $\hat{x}_t = \sum_{i = 1}^M a_i x_{t - i}$.
Minimizing FPE is equivalent to minimizing the quantity: 
\begin{equation}
    \mathcal{L}_{\rm FPE}(m) = P_{m} \frac{N + m + 1}{N - m - 1}
\end{equation}
with $P_m$ defined by the prediction error filter equation \ref{eq:errorFilter1}. \\ 
If the $N \to \infty$ limit is considered,remembering $m_{max} = 2N / \log(2N)$, we see that N dominates both the numerator and the denominator of the fraction: in large $N$ limit using Akaike's loss function is equivalent to the minimization of the variance $P_m$ of the process. 

\item \textbf{Criterion Autoregressive Transfer function (CAT)}
This second loss function has been written by Parzen and studied into details by Banshali ~\cite{bhansali1986}. 
It is based on the assumption that the observed process is an infinite order autoregressive process 
\begin{equation} 
x_t = \sum_{i = 1}^{\infty}a_{i}x_{t-i} + \nu_t
\end{equation} 
and tries to select the order $m$ as the best finite-order approximation for the observed process. Being N the number of samples it has the property that $m \to \infty$ as $N \to \infty$. Since any real-valued stochastic process can be written as a ARMA(p,q) process (Wold theorem) i.e. an $AR(\infty)$ process, this is a physically significant property. 
The loss function has the functional form: 
\begin{equation}
    \mathcal{L}_{\rm CAT}(m) = \frac{1}{N}\sum_{k = 1}^m \frac{N - k}{N P_k} - \frac{N - m}{N P_m},
\end{equation}
and the so chosen order is found to be asymptotically unbiased for $P_N$. 

\item \textbf{Optimum Bayes Decision rule} 
The last criterion we will consider is the Optimum Bayes Decision rule (OBD)\cite{doi:10.1029/WR018i004p01097}. Let $x(t)$ be the observed 
time series for the process that has to be described as an autoregressive process of order $m$ (AR(m)) with m to be determined. The OBD is obtained chosing between M different hypothesis $H_i, i = 1, \dots, M$ maximizing their posterior distribution $P(H_i\vert x(t))$. Each
hypothesis is uniquely determined from both the length and the values of the filter under study. Chosing a Likelihood of the form \eqref{eq:p_forecast} and uniform priors for both the coefficients and the hypothesis Rao has shown that chosing the minimum for $-log(P(H_i \vert x(t))$, i.e. maximizing the posterior for $H_i$, is asymptotically equivalent to the minimization of \wdp{please check this equation}: 

   \begin{align}
        \mathcal{L}_{\rm OBD}&(m) = (N - m - 2) \log(P_m) \nonumber\\
        &+ m \log(N) + \sum_{k = 0}^{m-1} \log(P_k) + \sum_{k = 1}^{m} a_k^2\,.
    \end{align}
\end{itemize}

Once a loss function is selected, the choice of the best recursion order is straightforward: we solve the Levinson \cite{doi:10.1002/sapm1946251261} recursion until $M_{max}$, as given in Eq.~\eqref{eq:MMAx}, iterations are reached. Then, the order $m$ is selected to be the one that minimizes the loss function.

In a real implementation of the algorithm, computing all the recursion up to $M_{max}$ can result in a significant waste of computational power: the optimal value is often $m_{opt} << M_{max}$ and, in such cases, computing all the values of $m$ until $M_{max}$ is not useful.
In practice, we can apply an \textit{early stop} procedure: every few iterations we look for the best order of $m_{opt}$; if this value does not change for a while, we assume that a good (local) minimum of the loss function is found and the computation is stopped.

The following sections will be devoted to the study of the statistical properties of the loss functions introduced above: we need to understand which choice provides the best quality in the reproduction of some known power spectral densities. 

\subsection{Choice of the loss function: Gaussian PSD} \label{sec:arp_validation}
We test the performance of the three loss functions on a random time series generated with a known power spectral density.
In this first experiment, we take the PSD to be a Gaussian distribution with mean $\mu = 2.5 \quad units$ and standard deviation $\sigma = 0.5 \quad units$.
The time series are generated by sampling a frequency vector from ${p(\tilde{n}(f_i)) \propto \exp{\left\{-\frac{1}{2}\sum_i\frac{n_i ^2}{S(f_i)}\right\}}}$, following the procedure outlined in sec.~\ref{sec:forecasting}, and performing an inverse Fast Fourier Transform on the frequency series. 
For this investigation, we generate a dataset of 1000 time series of 3000 points each.

We then apply the Burg method choosing in turn each of the loss functions on an ensemble of simulated time series, 
thus obtaining an ensemble of PSD estimates, that will allows the statistical characterisation of the performance of each loss function.
The disagreement between the PSD $S_i(f)$ estimated from the $i$-th simulated time series and the target PSD $S(f)$ is measured via 
the relative error $r_i$, averaged over all frequencies:
\begin{equation}\label{eq:freq_error}
r_i = \frac{1}{Ny}\sum_{f_j=0}^{f_j = Ny} \frac{|S_i(f_j) - S(f_j)|}{S(f_j)}
\end{equation}
where $Ny$ is the Nyquist frequency of the time series.

For each loss function, we compute the ensemble-averaged PSD (as well as the $90\%$ confidence level) and the ensemble-averaged frequency dependence of the error:
\begin{equation}\label{eq:ensemble_averaged_error}
	r(f) = \sum_i \frac{|S_i(f) - S(f)|}{S(f)} \;.
\end{equation}
Figs.~\ref{fig:FPEmean} \ref{fig:OBDmean} and \ref{fig:CATmean} show the averaged PSD for the three loss functions, 
as well as the ensemble-averaged error from Eq.~\eqref{eq:ensemble_averaged_error}.
Furthermore, Fig.~\ref{fig:optcomparison} displays the correlation between $r_i$ and the autoregressive 
order $m_i$ chosen for each independently drawn time series.

%In section~\ref{sec:LIGO_validation}, we will perform the analysis again using the Advanced LIGO design sensitivity PSD \cite{Ligo}.


\begin{figure}[t]
	\centering
	\includegraphics[width = \linewidth]{Images/optimisers_comparison/normal/FPE_spectrum_estim.pdf}
	\caption{Average spectrum for $\mathcal{L}_{\rm FPE}(m)$ with 90\% confidence regions (top) and percentage error (bottom). The average is computed with $1000$ realization of a $3000$ points long time series.}
	\label{fig:FPEmean}
\end{figure}
\begin{figure}[t]
	\centering
	\includegraphics[width = \linewidth]{Images/optimisers_comparison/normal/OBD_spectrum_estim.pdf}
	\caption{Average spectrum for $\mathcal{L}_{\rm OBD}(m)$ loss function with 90\% confidence regions (top) and percentage error (bottom). The average is computed with $1000$ realization of a $3000$ points long time series.}
	\label{fig:OBDmean}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width = \linewidth]{Images/optimisers_comparison/normal/CAT_spectrum_estim.pdf}
	\caption{Average spectrum for $\mathcal{L}_{\rm CAT}(m)$ loss function with 90\% confidence regions (top) and percentage error (bottom). The average is computed with $1000$ realization of a $3000$ points long time series.}
	\label{fig:CATmean}
\end{figure} 

\begin{figure}
	\centering
	\includegraphics[width = \linewidth]{Images/optimisers_comparison/normal/error_length_contour.pdf}
	\caption{Relative error $r$ (as in eq.~\eqref{eq:freq_error}) vs the length $m$ of autoregressive process, for each of the $1000$ independent realizations of the synthetic time series with a gaussian PSD. Different colours refers to different choices for the loss function: in red $\mathcal{L}_{\rm FPE}(m)$, in green $\mathcal{L}_{\rm OBD}(m)$ and in black $\mathcal{L}_{\rm CAT}(m)$. The top and right histograms show the marginal distributions.}
	\label{fig:optcomparison}
\end{figure}

\paragraph{Final Prediction Error (FPE)}
The results of our investigation on the performance of FPE are shown in Fig.~\ref{fig:FPEmean}.
Qualitatively, there is a good agreement between the reconstructed spectrum and the true spectrum; 
however, we note that the reconstruction is not very accurate in the low frequency region. 
Furthermore, the $90\%$ credible region is very small: this means that if we randomly take two of the reconstructed spectra, we expect their differences to be statistically small. These facts are evidence for FPE to be a reliable loss function.

By looking at therRed series in Fig.~\ref{fig:optcomparison}, we note that the autoregressive orders obtained with FPE are clustered in a very small region. This is a desirable property: FPE, in fact, provides a stable estimation of the autoregressive order, which does not affect much the reconstruction error.
We conclude that the FPE shows good quality reconstruction for the spectrum and very desirable stability properties, its estimate for filter's length $m$ is clustered in a region where there is no dependence of error from $m$. 
\paragraph{Optimum Bayes Decision Rule (OBD)}
The second loss function in consideration is OBD. As for the FPE loss function, the results in Fig.~\ref{fig:OBDmean} show a 
good agreement between the average over the reconstructions and the true spectrum.
Qualitatively the same behaviour of FPE is observed: a good quality reconstruction at the intermediate and high frequencies with a low $90\%$ confidence level; and a worse performance at low frequencies.
However, when looking at the error, the disagreement of this method is found to be larger than the one obtained with FPE: in the worst case, the error can be as large as a factor of $2$ when compared with FPE. 

The error as a function of the autoregressive length (green series in Fig.~\ref{fig:optcomparison}) cluster in a small region, indicating the stability of the reconstructed process order $m$. We note that on average, OBD tends to choose smaller values of $m$ with respect to FPE.

\paragraph{Criterion Autoregressive Transfer function (CAT)}
The performance of CAT is shown in Fig.~\ref{fig:CATmean}. At a first glance CAT differs substantially from the other two loss functions considered.
The ensemble-average PSD matches very well the underlying ``true" PSD. This is also true even in the low frequency region, 
where both OBD and FPE had a poor performance.
However, the variance of the reconstructed spectrum is quite large (much larger than for FPE and OBD), and the relative error is quite high, $\sim 10\%$ and it is approximately constant over all the frequencies.

The reason for this behaviour becomes clear from Fig.~\ref{fig:optcomparison} (black series). CAT does not converge 
to any specific value for the order of the autoregressive process.
In turn, the estimates for the order span a large range of values, hence the large variance observed.
Fig.~\ref{fig:optcomparison} also shows a strong dependence of the error on the estimated length of the filter. The good quality of the reconstruction from the average spectrum can be explained as follows: long filters are able to capture features that short filters cannot see, like outliers in different realization of the time-series, but this is also responsible of an increased variance in the estimate, by introducing spurious peaks in the reproduction.

When averaging the different PSD estimates, the noise in each spectrum cancels, as expected for Gaussian noise. 
This implies, in a sense, that each estimate of the spectrum is independent of any other, as suggested by the huge variance in the residuals. This lack of stability is not a good property for the estimation estimation of the PSD from a single realisation of the time-series, however, thanks to the averaging out of the errors, this estimator seems optimal in the case of repeatable experiments and ensemble-averages.

\paragraph{Final remarks on the choice of the loss function}

In our analysis, the FPE and OBD loss functions are found to behave similarly while CAT shows fairly different properties.
CAT provides an accurate average spectrum over all the frequencies at the price of a large variance; in turn OBD and FPE 
provide a poorer average in the low frequency tails, however they also display a  smaller variance, with FPE showing the lowest.

The poor low-frequency reconstruction from OBD and FPE might be due to the fact that very small values require longer filters. 
In fact, when we consider the average for all the spectra we note that short filters yield larger the error in the tails. OBD, which select the shortest filters, can provide an error as large as $300 \%$ at the extrema, as compared with $85\%$ error of FPE. In turn, CAT is $5\%$ accurate in those regions.

However, we also note, see Fig.~\ref{fig:optcomparison}, that the psD inferred from a single time-series realisation with FPE is still able 
to provide the lowest averaged error over all frequencies, while CAT can reach errors $5$ times larger.
Hence, while CAT is the loss function that minimizes errors in the low-frequency end of the spectrum, 
FPE obtains the best overall accuracy. 

The conclusion is clear: even if in some cases CAT might be more accurate when taking the average over several estimation, FPE guarantees that the single estimation will be more accurate.
As in any common situation we cannot perform such averaging over different realization of the same time series, we must prefer FPE over CAT (let alone OBD, which even though qualitatively similar to FPE has worse performance).
However, if we indeed can measure the PSD by averaging over different time series, using CAT as a loss function is the best choice. In this sense
we retain CAT to provide the best, and most similar in spirit, alternative to the commonly employed WelchPsd estimation method whenever 
ensemble-averages are needed and justified.

%It is evident that tails do not affect CAT  average error and standard deviation at all, so that it's accuracy is approximately constant everywhere in the spectrum. \\ \ 
%Since both FPE and CAT shows pros and cons, at this level, there is no reason to choose between them.\\ 
%Analyzing the resolution of the single reconstruction, figure \ref{fig:optcomparison} clearly shows that FPE minimize overall relative error giving a stable result.\\ 
%CAT instead shows a larger error and do not converge to any specific value. Almost every reproduction obtained with CAT is worst with respect to those obtained with FPE, but their mean is more precise. \\ 
%It's instability is probably the cause for this behaviour. Since it does not show a strong convergence to some order with respect to others, very different lengths are selected.
%Longer lengths are able to capture a lot of details but are more likely to introduce noise. In other words, long filters reduce bias but increment the variance of the result. 
%Taking the average, for central limit theorem the error decreases as $\sim \frac{1}{\sqrt{N}}$. In this way we compensate noise and obtain a spectrum with small bias and small fluctuations.
%Considering the average, CAT reconstruct precisely and with good accuracy every part of the spectrum, even if in "regular" parts best accuracy is reached from FPE. For a single reproduction, CAT is very noisy and unstable, while FPE performs the better. \ref{fig:optcomparison}
%definitely shows that FPE gives the best results: it is a stable method that choose filters in the clustered area associated with minimum error, while both OBD and CAT result lies outside this area and show bigger errors. Two other interesting plots that stress this fact are reported in figure \ref{fig:ordersCompairson} and \ref{fig:residualsComparison}
%where the histograms for the errors and for order's estimate are reported. These graphs provide stronger evidence for our previous statements. As already noticed, FPE is very stable and provide the smallest 'overall relative error'. \\
%CAT histogram shows a very interesting behaviour. The histogram for the order show two different peaks,  a small one associated to 'short filters', and a second high peak at $m = M$. This means that CAT is most likely to not converge at all at some value for the regression order, and it is more likely selects a length that is comparable with the largest available.  If short filters are selected, the overall percentage error will be low, respectively represented by the first peak in the distribution of the errors, while if long filters are selected, the error is maximized. 



\subsection{Choice of the loss function: LIGO Spectrum} \label{sec:LIGO_validation}
We continue our characterisation of the various loss functions considered in this work, by investigating the 
reconstruction of a specific, known, power spectrum: that is the Advanced LIGO design sensitivity theoretical spectral curve\wdp{add ref}.
For this analysis, we generate $500$ time series of $40960$ points each, sampled with a sampling rate of $\SI{2048}{Hz}$, hence we fix the duration
of the time-series to $\SI{20}{s}$. The chosen length is convenient to capture fairly accurately the low-frequency features of the LIGO PSD.

\begin{figure}
    \centering
     \includegraphics[width = \linewidth]{Images/optimisers_comparison/ligo/compare_estimates.pdf}
      \caption{Average spectrum for the three different loss function as compared with the ``true" PSD. The average is computed with $500$ realization of a $40960$ points long time series.}
       \label{fig:ligospectrum}
\end{figure}
Our findings are summarised in Figs.~\ref{fig:ligospectrum} and ~\ref{fig:LigoOrderError}. The former shows the original spectrum (dashed line)
and the ensemble-averaged reconstructed PSD adopting the FPE (green line), OBD(red line) and CAT(black line). In all cases, the spectrum is well reconstructed, but with a fairly distinct behaviour at low frequency, where CAT -- as in the Gaussian case -- better captures and resolves the distinct 
spectral feature at $\sim 17$ Hz, see also the left panel in Fig.~\ref{fig:ligoPeaks}, as well as the high frequency line at $438$Hz, right panel in Fig.~\ref{fig:ligoPeaks}. 
Fig.~\ref{fig:LigoOrderError} shows the distribution of recovered AR orders $m$ against the relative frequency-averaged error. The behaviour
of the three loss functions is very similar to what found in the Gaussian PSD case: ODB infers the smallest orders and gives average errors around $20$\%, FPE consistently estimates orders of a few hundreds and shows the smallest errors $\sim 15$ \%while CAT does not show any preference towards any AR order and displays wildly varying errors. Yet again, when the PSD is averaged over multiple realization of the time, CAT is able to capture the spectrum very precisely. In fact, even in presence of very sharp peaks, CAT reconstruction seems to be almost perfectly coincident with each of them (figs.\ref{fig:ligoPeaks}).
Hence, also the study of simulated Advanced LIGO data seems to indicate that whenever and wherever ensemble-averaged PSD are necessary, CAT is the optimal choice of loss function. However, on a single time-series realisation, FPE is the more robust choice.

Let us summarise some key general conclusions:
\begin{itemize}
	\item There are no reasons to prefer OBD over CAT or FPE.
	\item If we have one single realization for the process, we recommend the use of FPE, that would get the best resolution possible. In this situation, CAT would provide spurious and unreliable results, with large error.
	\item In the case of several realizations of the same process, CAT ensemble-average properties provide very precise spectral estimation.\end{itemize}

Therefore, the choice of loss function, at least in between CAT and FPE, depends on the problem one  is attempting to solve.
\begin{figure}[t]
        \includegraphics[width = \linewidth]{Images/optimisers_comparison/ligo/compare_estimates_peaks.pdf}
        \caption{Details of peaks of the spectrum and their reconstruction with every optimizer}
        \label{fig:ligoPeaks}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width = \linewidth]{Images/optimisers_comparison/ligo/error_length_contour.pdf}
    \caption{For each of the $500$ independent realization of the time series, we plot the relative error $r$ (as in eq.~\eqref{eq:freq_error}) against the length $m$ of autoregressive process. The time series are randomly drawn with a the analytical LIGO PSD in fig.`\ref{fig:ligospectrum}. Different colors refers to different choices for the loss function. Histograms for the distribution od the individual quantities are also represented.}
    \label{fig:LigoOrderError}
\end{figure}

\subsection{How well the autoregressive order is recovered?}

\begin{figure}
	\centering
	\includegraphics[width = \linewidth]{Images/arp_errors/scatter_deltap_ptrue}
	\caption{Reconstructed value for the autoregressive order plotted against the true value of the autoregressive order.
	The reconstructed autoregressive orders are computed from a time series randomly drawn with an $AR(p)$ model, with the three different loss functions under investigation.
	}
	\label{fig:p_vs_ptrue}
\end{figure}

We now address the issue of how well the autoregressive order (i.e. the number of $a_k$ coefficients employed) is estimated with each loss function.
In doing so, we generate $100$ autoregressive processes $AR(p)$ with a random value of $p$, drawn such that $log(p) \sim\mathcal{U}_{[log(2), log(5000)]}$.
Each coefficient $a_k$ is assigned according to a Dirichlet distribution $\mathrm{Dir}([1,\hdots, 1])$. The sign of $a_k$ is assigned randomly according to a binomial distribution.
We report the result of this investigation in fig.~\ref{fig:p_vs_ptrue}.

It can be seen that OBD and FPE loss functions show very similar behaviour. They are {\it very} realiable in capturing the correct autoregressive order up to a certain threshold ($p\sim 100$ for OBD and $p\sim 200$ for FPE). For any autoregressive process of order higher than the threshold, the optimization underestimates badly the actual value of $p$.
For this reason, FPE and OBD seems realiable only for simple autoregressive processes. For complicated processes, it seems they underfit the problem (i.e. they output a model simpler that those required).

On the other hand, CAT loss function shows a different behaviour. The selected autoregressive order $p$ is always close to the maximum possible value, regardless the actual value $p_{true}$ of the underlying process. This produces always a model that is more complex than those obtained with FPE and OBD and this can explain the origin of the high variance in the estimation observed in the experiments described in the previous section.
Of course, CAT does not perform a good job in reconstructing an autoregressive process. However, for high order autoregressive process the error introduced by CAT is more tolerable than the error introduced by FPE and OBD. This is a good reason to prefer CAT optimizer in such situations (see sec.~\ref{sec:temp_timeseries} for another example of this).

\sschmidt{Shall we add more?} 

\section{Comparison with Welch method}\label{sec:Welch_comparison}

It is really interesting to have a qualitative comparison between the performance of the MESA and of the standard Welch algorithm.
We perform such comparison on the spectrum of LIGO (even though similar conclusions can be drawn from any other PSD).

We perform two sets of experiments.
In the first one (see fig.~\ref{fig:mem_welch_fakedata}), we simulate data from the realesed LIGO intererferometer PSD and we make the comparison with them: this is to ensure the we have a baseline PSD to compare the data with.
In a second experiment (see fig.~\ref{fig:mem_welch_realdata}), we employ the data released to the public by the LIGO/Virgo collaboration \cite{Abbott_2021}.
In each experiment, we vary the length of the data used for the estimation: this is also useful to assess how the computation depends on the data available. We set the total observation time $T = 1, 5, 10, 100, 1000 \SI{}{s}$
For the MESA algorithm, we employ FPE optimizer. For the Welch algorithm, we employ a Tukey window, an overlap fraction of $0.5$ for the segments and a length of segments $L = 512, 1024, 2048, 8192, 32768$ datapoints.
In all cases, the sampling rate is set to $\SI{4096}{Hz}$.

First of all, we note that using a longer time series results in a more stable estimation of the PSD, especially in the low frequency sector. This is somehow obvious: more data provide more information available for spectral estimation. This fact is more important at lower frequencies, where the natural time length is large.

Turning to the comparison between MESA and Welch's method, we find that the PSD estimation provided by the Welch's method is more noisy (i.e. has a large number of spurious peaks) rather than PSD provided by MESA. The different behavior is more manifest in the low frequency sector.
The result does not change much by changing the hyperparameters required for Welch algorithm.
\sschmidt{Elaborate a bit more on this... Right now it is a very poor comment (blame is on me!)}


\begin{figure}
	\caption{Comparison between PSD estimation with Welch method (blue) and Maximum Entropy (red) for \textit{synthetic} LIGO data. The estimation is based data with an increasing observation time $T = 1, 5, 10, 100, \SI{1000}{s}$. In dashed black is reported the true PSD (from which data are generated).}
	\label{fig:mem_welch_fakedata}
	\includegraphics{Images/comparison_LVC_data/comparison_LVC_data_overall_fake_True.pdf}
\end{figure}

\begin{figure}
	\caption{Comparison between PSD estimation with Welch method (blue) and Maximum Entropy (red) for \textit{real} LIGO data. The estimation is based data with an increasing observation time $T = 1, 5, 10, 100, \SI{1000}{s}$.}
	\label{fig:mem_welch_realdata}
	\includegraphics{Images/comparison_LVC_data/comparison_LVC_data_overall_fake_False.pdf}
\end{figure}


\section{Applications} \label{sec:applications}
\subsection{Temperature Time Series} \label{sec:temp_timeseries}
We apply our code to the study of the temperature time series. This will allow to test the code performance on a different time series, of great importance in physics. We will also assess the quality of the forecasting predictions: this task has a great impact also on practical applications.
We used the historical reanalysis data from the ``ERA5-Land hourly data from 1981 to present" dataset, downloaded from the Climate Data Store \cite{ERA5_dataset}

Data refers to the temperature measure at coordinates $\textrm{N}\;\ang{45;5;} \;\; \textrm{E}\;\ang{9;1;}}$, corresponding about to the city of Milan, Italy. The temperature is measured once per hour for almost 31 years from 31st December 1989 to 30th November 2020.

In fig.~\ref{fig:temp_spectrum}, we compute the spectrum over all the data. We employ both FPE and CAT loss functions.
First of all, we note that at every frequency the estimation given by FPE is much less noisy the one from CAT: this issue was already studied in the previous sections. 
We clearly see that there is a peak on the PSD at the frequency $f_D = \SI{1}{day^{-1}}$ corresponding to a day. This is somehow trivial and highly expected.
Higher order harmonics (corresponding to integer multiples of $f_D$) are also visible up to the Nyquist frequency ($f_D = \SI{0.5}{hour^{-1}}$): they corresponds to signals that preserve the 1 day period while, at the same time, capturing the complex variability of the daily temperature cycle throughout the year.

Looking at the low frequencies, it is striking to note that FPE does not capture the peak $f_{yr} = \SI{1}{yr^{-1}}$ located at the frequency of a year. On the other hand, the peak is captured well by the CAT loss function.
In analogy as what observed for the peak at $f_D$, in the CAT spectrum we observe a higher order harmonic at a frequency $2 \cdot f_{yr}$. As in the previous case, this is required to better model the temperature variation in the year.

The failure of FPE in catching the low frequency trend can be understood by looking at fig.~\ref{fig:p_vs_ptrue}. It is shown that CAT systematically select very long filters (i.e. large $p_{CAT}$ values for the autoregressive orders), whereas FPE tends to select smaller values for the order $p_{FPE}$ of the autoregressive process, often understimating badly the actual value.
In this example, we have $p_{CAT} \sim  36500 \sim  \SI{4}{yr}$, whereas $p_{FPE} \sim  1300 \sim  \SI{2}{months}$.
Clearly, the autoregressive process selected by FPE cannot produce a meaningful prediction on the timescale of a few months and for this reason it is unable to capture the peak at $f_{yr}$. In other words, the model is too simple to capture \textit{both} the behaviour at high and low frequencies (separated by approximately 3 orders of magnitude).
On the other hand, the model chosen by CAT is much more complex and is able to model also the high frequency behaviour.
The ``actual" length of the autoregressive process should be closer to the choice of CAT.

\sschmidt{
Some remarks:
\begin{itemize}
	\item What happens if we downsample the data? FPE should be able to capture the yearly trend...
	\item Remove yearly trend from the data?
	\item Other plots?
\end{itemize}
}

We now assess the accuracy of the forecasting of new observations of the time series.
Based on actual data, we try to predict future values as described in sec.~\ref{sec:forecasting}. We produce $N =100$ indepedent predictions and we compute the median as well as the $90\%$ confidence interval. This is compared with the actual measured temparature values.
The predictions spans a two years range of time.
We report our results in fig.~\ref{fig:temp_spectrum}.

We note the observed difference is always well included in the $90\%$ confidence interval: the forecasting predictions seem reliable.
On the other hand, the confidence interval is pretty large (almost as large as $\SI{15}{K}$), thus making ``easy" for the actual data to fit the predictions.
Indeed, the prediction model, while suitable for spectral estimation, is nothing more than a linear regression (plus noise term). Such a simple model hardly catches the complex trend in the variability of the temperature daily trend during a year. For more precise predictions, probably one should consider nonlinear regression model, tapping into the wealth of non linear predictors offered by the field of Deep Lerning.


\sschmidt{Data should be divided in train and test set for the purpose of forecasting!!}

\sschmidt{Other comments below??}

\begin{figure}
	\caption{Spectrum for the historical temperature time series. The two lines refers to different loss functions (CAT and FPE); the inset shows the harmonics of the fundamental frequency of a day. Two vertical lines are drawn in correspondence to the frequency $f_D$ of a day and $f_{yr}$ of a year.}
	\label{fig:temp_spectrum}
	\includegraphics{Images/climate_plots/temp_spectrum.pdf}
\end{figure}

\begin{figure}
	\caption{Difference between the historical temperature $T$ and the forecast temperature $T_{forecast}$ for two years of data. The red shadow denotes the $90\%$ confidence interval of the predictions. The model is trained with CAT loss function. The discrepancy between predicted and actual values is always everywhere included in the $90\%$ confidence interval.}
	\label{fig:temp_forecast}
	\includegraphics{Images/climate_plots/forecast_accuracy.pdf}
\end{figure}

\subsection{Seismic waveforms} \label{sec:seismic_timeseries}
\blindtext

\cite{seismic_data}

\blindtext

\begin{figure}
	\caption{Do a caption}
	\label{fig:seismic_spectrum}
	\includegraphics{Images/seismic_plots/seismic_spectrum.pdf}
\end{figure}

\begin{figure}
	\caption{Do a caption}
	\label{fig:seismic_forecast}
	\includegraphics{Images/seismic_plots/forecast_accuracy.pdf}
\end{figure}


\subsection{Forecasting the LIGO strain} \label{sec:LIGO_forecasting}
As a last example of and application of the maximum entropy method on real data, we come back to the strain produced by the LIGO-Virgo interferometers and we forecast the future observations. The spectral analysis is performed with CAT and FPE loss functions on $\SI{1000}{s}$ of data \cite{Abbott_2021} sampled at $\SI{4096}{Hz}$ observed at Livingston observatory starting from GPS time $1164603392$.
We then forecast the following $\SI{100}{s}$ of observations with the model optimized with CAT. The results are shown in fig.~\ref{fig:LIGO_forecast}.

By looking at the figure, we note that the forecasted prediction is always in the $90\%$ confidence interval.
The confidence interval, however, is very broad and the prediction is not very accurate (being the same order of magnitude of the strain).
By looking at the standard deviation $\sigma$ of the predictions (bottom panel in fig.~\ref{fig:LIGO_forecast}), we note that it increases very quickly in the first $\SI{0.5}{s}$ and then its trend stabilizes to a constant value.
The order of the autoregressive process selected is $p_{CAT} = 57766 \sim \SI{14}{s}$, which is much larger than the region in which $\sigma$ is small $\sim \SI{0.2}{s}$. This means that the series is very difficult to predict: the knowledge of past observations is not very helpful in predicting future observations.
Similar performances in the sigma are observed also with the FPE model: in this case FPE selects a large prediction filter $p_{FPE} = 29924$ , similar to those selected by CAT.

Despite poor predictions on long timescales, the autoregressive process obtained with MESA still can be useful on short timescales. Indeed, a precise prediction of the strain time series can be beneficial in anomaly detection and removal of glitches. The predictions can form an expected baseline for the strain; any anomaly (i.e. a glitch or even a transient of physical origin) can show up as a large departure from expected trend.
Moreover, if a glitch is detected, its shape can be estimated (as well as the confidence level) by subracting the expected signal with the actual signal.
This can work: a typical glitch \cite{Zevin_2017} can last as long as $\SI{0.2}{s}$, which is close to the time scale in which the forecasting is realiable.

As noted above, for pushing further the prediction performance from our simple linear predictor, more sophisticated forecasting methods are available in the Machine Learning literature (see e.g. \cite{LSTM_article, oord2016wavenet}) and they can be trained for precision forecasting. However, a Maximum-Entropy trained autoregressive process can be a simple baseline for comparing such more advanced methods and might suffice for many purposes.
This opens a promising path in GW data analysis and other fields of physics might also take advantage from this.

\begin{figure}
	\caption{In the top panel, we show the difference between the forecasted strain values (CAT model) and the actual values observed at Livingston interferometer (blue). The red shadow denotes the $90\%$ confidence interval subtracted with the actual time series values. The confidence interval is computed on  $100$ independent random realization of the predictions.
	The bottom panel displays the standard deviation of the predictions with both CAT and FPE models. The inset shows a detail of the first $\SI{0.5}{s}$ of plot in the top panel.
	}
	\label{fig:LIGO_forecast}
	\includegraphics{Images/forecast_LIGO/forecast_accuracy.pdf}
\end{figure}


\section{Final remarks and future prospects} \label{sec:conclusion}

We presented a case study of the application of Maximum Entropy principle in the realm of spectral estimation: it provides a closed form estimation of the power spectral density, bridging the spectral analysis with the study of autoregressive processes. The method has the towfold advantage of avoiding any unjustified assumption on the observed time series and of providing a simple context for forecasting the time series. We provided a ready to use python code, called `memspectrum' and we performed numerical studies of its performance.
We investigated the use of different loss functions for the purpose of correctly estimating the autoregressive order for the estimation and we found that different loss functions are required for different situations.
Finally, we performed a comparison between PSDs computed with MESA and standard Welch's algorithm and we applied our method to several time series of interest in physics.

As outlined in sec.~\ref{sec:Welch_comparison}, the MESA provides PSD estimates with less noise than the standard Welch algorithm, thus making it a good alternative for it. The use of MESA is particularly useful for short time series samples, where Welch method is outperformed in accuracy and conficence.
It has to be said that our method can be quite computationally expensive, as compared to standard Welch's algorithm. However, for short time series, the computational cost is still managable.

Besides providing an alternative to standard methods of spectral estimate, MESA opens up several oppurtunities in the context of time series analysis.

First of all, as short time series are required to produce meaningful PSD estimations, it can be suitable in the context of PE...
\textbf{##############PE part here#############}

Furthermore, MESA provides a simple, but robust and quite accurate, predictor for the time series. This fact is remarkable and can be used in time series analysis for several purposes.
As already discussed in sec~\ref{sec:LIGO_forecasting}, an anomaly detection pipeline can be easily built using the forecasts of MESA: the predictions can form a baseline to compare the actual observations with. Whenever the observed data are outside the expectations, an anomaly detection can be claimed.
Of course such predictions can be done with a more accurate (perhaps nonlinear model); however MESA has the advantage of being simple and fast to train, while providing good (short term) predictions.

\sschmidt{Other stuff with forecasting?}

We believe that Maximum Entropy Spectral Analysis, despite being an ``old" theory, has still the potential to become the standard in future time series analysis, due to its theorical elegance and to its reliable performance in a vast range of application.



	\begin{acknowledgments}
%We want to thank Maurizio Maugeri and Costanza Rossi for useful discussions.
\sschmidt{Is it fine to ackonowledge people helping with temperature data and seismic data?}

This research has made use of data, software and/or web tools obtained from the Gravitational Wave Open Science Center (\url{https://www.gw-openscience.org/}), a service of LIGO Laboratory, the LIGO Scientific Collaboration and the Virgo Collaboration. LIGO Laboratory and Advanced LIGO are funded by the United States National Science Foundation (NSF) as well as the Science and Technology Facilities Council (STFC) of the United Kingdom, the Max-Planck-Society (MPS), and the State of Niedersachsen/Germany for support of the construction of Advanced LIGO and construction and operation of the GEO600 detector. Additional support for Advanced LIGO was provided by the Australian Research Council. Virgo is funded, through the European Gravitational Observatory (EGO), by the French Centre National de Recherche Scientifique (CNRS), the Italian Istituto Nazionale di Fisica Nucleare (INFN) and the Dutch Nikhef, with contributions by institutions from Belgium, Germany, Greece, Hungary, Ireland, Japan, Monaco, Poland, Portugal, Spain.
	\end{acknowledgments}

\pagebreak
\appendix
\section{Details of PSD computation} 
\subsection{MESA solution}\label{sec:MESA_solution}
We derive the expression for the MAXENT spectral estimator following the approach proposed by Burg \cite{burg1975maximum}.
Unlike the standard approach, we do not enforce the constraints in eq.~\eqref{eq:MaxConstraint} with the standard Lagrange Multipliers approach.
We write instead the PSD $S(f)$ as the Fourier Transform of the sample autocorrelation function: 
\begin{equation}
    S(f) = \frac{1}{2 Ny}\sum_{n = -\infty}^{\infty} \bar r_n e^{- \imath 2 \pi n \Delta t},
\end{equation}
and the entropy gain expression eq.~\eqref{eq:EntropyGain} becomes:
\begin{equation}
    \Delta H = \int_{-Ny}^{Ny}  
    \log\left(\frac{1}{2 Ny}\sum_{n = -\infty}^{\infty} \bar r_n e^{-\imath 2 \pi f n \Delta t} 
    \right) df.
\end{equation}
Note that this expression already takes into account the contraints in eq.~\eqref{eq:MaxConstraint}.

We now introduce a set of coefficients $\lambda_s$, defined as the derivative of $\Delta H$ with respect to the autocorrelation function $r_s$.
Excplitely they are:
\begin{equation} \label{eq:lamdas}
      \lambda_s \coloneqq \frac{\delta H}{\delta \bar r_s} = \frac{1}{2Ny}\int_{-Ny}^{Ny} S(f)^{-1}e^{-\imath 2 \pi f s \Delta t } df
\end{equation} 
and we will show that $S(f)^{-1}$ can be written as a Fourier Expansion in terms of such coefficients. Thenm the determination of the values for the $\lambda_s$ uniquely solves the problem of power-spectral density estimation.

Some properties for the coefficients can be worked out easily. First, since $S(f)$ is positive definite the $\lambda_s$ show the property 
\begin{equation}
	\nonumber 
	\lambda_s = \lambda_{-s}^*. 
\end{equation}
The second property is obtained considering that the autocorrelation function $r_n$ can only be computed for a finite time interval $n \in [-N, N]$ and that the PSD estimation must not depend on the unavailable values $r_n$: this is part of the constraint in eq.~\eqref{eq:MaxConstraint}
This requirement can be implemented as:  
\begin{equation}\nonumber 
    \frac{\delta H}{\delta \bar r_s} = 0 \text{ for } \vert s \vert > N,
\end{equation}
that means 
\begin{equation}
\nonumber 
\lambda_s = 0 \text{ for } \vert s \vert > N. 
\end{equation}

From eq.~\eqref{eq:lamdas} and from the properties above, is easily seen \sschmidt{How exactly?} that $S(f)$ can be expressed via a Fourier Series 
\begin{equation}\label{eq:PSDconstraint}
    S(f)^{-1} = \sum_{s = -N}^N \lambda_s e^{-\imath 2 \pi f s \Delta t}.
\end{equation}
Defining $z = e^{-\imath 2 \pi f \Delta t}$ the previous Fourier expansion becomes a Laurent Polynomial in $z$: 
\begin{equation}
    \label{eq:zExp}
    S(f)^{-1} = \lambda_0 + \sum_{s = 1}^N \lambda_s z^s + \sum_{s = 1}^N \lambda^*_s z^{-s}.
\end{equation}
It is easy to show that if $z_0$ is a root for the polynomial $(z_0^*)^{-1}$ is also a root: for every root laying outside the unit circle there will be another root inside of it and vice-versa. These properties allow us to rewrite the Fourier expansion \eqref{eq:zExp} as \cite{1975STIN...7714318B}:
\begin{equation}\label{eq:MESApsd_appendix}
    S(f) = \frac{P_N \Delta t}{\left(\sum_{s=0}^N a_s z^z\right)\left(\sum_{s = 0}^N a^*_s z^{-s}\right)}
\end{equation}
with $a_0 = 1$ and $\Delta t$ the uniform sampling interval for the time series. The vector obtained as $(1, a_1, \dots, a_N)$ is the prediction error filter. The power spectral density $S(f)$ is uniquely determined if both the prediction error filter and $P_N$ coefficients are computed.

To compute the $a_s$ is convenient to plug into eq.~\eqref{eq:MaxConstraint} the Laurent Polynomial exansion for $S(f)$ eq.~\eqref{eq:MESApsd_appendix} and then integrating over $z$. In this way the equation becomes:
\begin{equation}
    \label{eq:contourintegral}
   \frac{P_N}{2 \pi \imath} \oint _{\mathbb S^1}\frac{z^{-s - 1}}{\sum_{n = 0}^N a_n z^n \sum_{n = 0}^N a^*_n z^{-n}}dz = \bar r_s. 
\end{equation}
Substituting $s \to s - r$, multiplying by $a^*_s$ and summing over $s$, the previous equation becomes 
\begin{equation}
    \label{eq:errorFilter}
    \sum_{s = 0}^N a_s \bar r_{s - r} = \frac{P_N}{2 \pi \imath}\oint \frac{z^{r -1}}{\sum_{s = 0}^N a_s z^s}dz
\end{equation}
For a wide-sense stationary processes \sschmidt{Justify this assumption!}, all the poles lay outside the unit circle so that the previous integral can be easily computed obtaining the following, well known, equations: 
\begin{align}
    \label{eq:errorFilter1}
    \sum_{s = 0}^N a_s \bar r_{r - s} &= P_N \quad \text{ if } r = 0 \\ \label{eq:errorFilter2}
    \sum_{s = 0}^N a_s \bar r_{r - s} & = 0 \qquad \text{ if } r \neq 0.
\end{align}

\subsection{Levinson recursion} \label{sec:LevinsonRecursion}
The solution of the eqs.~(\ref{eq:errorFilter1}-\ref{eq:errorFilter2}) fully determines the functional form of the power spectral density estimator \eqref{eq:MESApsd_appendix}.
The method for solving the equations is called the Levinson-Durbin recursion \cite{doi:10.1002/sapm1946251261} and it is described in the following.
For each order $N$ of the iteration we define the quantities:
\begin{align}
\Delta_N &= \sum_{n = 0}^{N} a_n \bar{r}_{N - n + 1} \\ 
c_N &= - \frac{\Delta_N}{P_N},
\end{align}

The Levinson recursion computes the $N$th order quantities given the $N-1$th order quantities: 
\begin{equation} \label{eq:Levinson1}
P_N = P_{N -1}\left(1 - \vert c_{N - 1} \vert ^2\right)
\end{equation}
and 
\begin{equation} \label{eq:Levinson2}
\begin{pmatrix}
1 \\ a_1 \\ \vdots \\ a_{N - 1} \\ a_N
\end{pmatrix}
= 
\begin{pmatrix}
1 \\ b_1 \\ \vdots \\ b_{N -1} \\ 0
\end{pmatrix}
+ c_{N-1}
\begin{pmatrix}
0 \\ b_{N -1}^* \\ \vdots \\ b^*_1 \\ 1
\end{pmatrix}. 
\end{equation}
where $b$ is the forward prediction error coefficients at order $N-1$. 
The 0-th order element can be easily initialized reminding that $a_0 = 1$ (always) and that $P_0$ can be determined from \eqref{eq:errorFilter1}.
Its values turns out to be: 
\begin{equation}
P_0 = R(0),
\end{equation}
$\Delta_0$ and $c_0$ are uniquely determined from their definitions and they are:
\begin{equation}
\Delta_0 = R(1); \quad c_0 = -\frac{R(1)}{R(0)}. 
\end{equation}

These expressions allow us to compute $\vec a$ and $P_N$ to any order by simply iterating \eqref{eq:Levinson1} and \eqref{eq:Levinson2}. Substituing them in equation \eqref{eq:MESApsd_appendix} the problem of the estimation for the power spectral density via maximum entropy principle is solved.
Burg's method for spectral analysis is solved via Levinson is implmenteed in the released \texttt{memspectrum} package.
Another faster recursion method is available in \cite{Vos} and it is also available in \texttt{memspectrum}.


	\bibliography{Bibliography.bib}
	\bibliographystyle{ieeetr}

\end{document}








